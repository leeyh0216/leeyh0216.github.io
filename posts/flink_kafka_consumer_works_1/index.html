<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8829030678254956" crossorigin="anonymous"></script><meta name="pv-proxy-endpoint" content=""><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Flink Concept - Flink의 Kafka Consumer 동작 방식(1)" /><meta name="author" content="leeyh0216" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="개요 Flink의 FlinkKafkaConsumer와 같은 High Level API를 사용하다보면 궁금해지는 점들이 있다. Kafka의 Consumer는 Pull 방식으로 데이터를 가져오는데, Flink에서는 얼마 정도의 간격으로 Pull을 수행할까? Source와 다른 Operator를 Chaining하면 Kafka로부터 데이터를 Pull할 때 해당 Operator Chain이 Blocking 될까?(즉, I/O와 Computing이 합쳐져 있을까, 분리되어 있을까?에 대한 질문) Kafka Transaction이나 발생하는 Exception 은 어떻게 처리할까? 이번 글에서는 Flink에서 Kafka로부터 어떻게 데이터를 가져오고, 이를 어떻게 Pipeline에 전달하는지 알아보도록 한다. Flink의 Kafka Consumer 동작 방식 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); 위의 코드는 Flink의 공식문서에서 제공하는 Kafka Source 사용 방법이다. FlinkKafkaConsumer 클래스 하나만 노출하여 단순해보이지만, 내부적으로 많은 클래스가 Flink와 Kafka Consumer 연동에 관여하고 있기 때문에 전체 구조를 파악한 뒤, 어떤 흐름으로 동작하는지를 알아보도록 한다. 사전 지식 Flink의 SourceFunction Flink에 Data Source를 연동하기 위해서는 SourceFunction(ParallelSourceFunction)을 구현해야 한다. Kafka 또한 FlinkKafkaConsumer(의 부모 클래스인 FlinkKakfaConsumerBase)가 이를 구현하고 있다. SourceFunction을 상속받아 구현해야 하는 주요 메서드는 run과 cancel이 있다. run: Pipeline이 시작될 때 한번 호출되며, cancel이 호출되기 전까지 내부에서 Loop를 돌며 데이터를 만들어내야 한다. cancel: Pipeline이 종료될 때 호출되며, run에서 실행되고 있는 Loop를 멈춰야 하는 책임을 가진다. Flink에서 제공하는 run과 cancel의 예제 코드는 다음과 같다. run 메서드 public void run(SourceContext&lt;T&gt; ctx) { while (isRunning &amp;&amp; count &lt; 1000) { synchronized (ctx.getCheckpointLock()) { ctx.collect(count); count++; } } } Loop가 계속 돌 수 있는지 체크하는 isRunning을 체크하며 Loop를 돌고 있고, run 메서드 호출 시 매개변수로 전달받은 SourceContext 객체인 ctx의 collect 메서드를 통해 다음 Operator로 데이터를 전달하는 것을 확인할 수 있다. 또한 CheckPoint가 수행될 때(synchronized)는 데이터를 만들어내지 않는 것을 확인할 수 있다.(추후에 확인해봐야겠지만 이 때 Checkpoint Barrier가 전달되는 것으로 예상된다) cancel 메서드 public void cancel() { isRunning = false; } run 메서드의 Loop에서 사용하는 isRunning 값을 false로 변경하므로써, SourceFunction의 동작을 멈추는 것을 확인할 수 있다. Architecture Overview Flink의 Kafka Consumer를 구성하는 전반적인 클래스 다이어그램이다. 주요 클래스와 그 역할은 아래와 같다. FlinkKafkaConsumerBase: Flink의 SourceFunction을 구현하며, 실제 데이터를 KafkaFetcher 객체를 통해 전달받는다. KafkaFetcher: KafkaConsumerThread에서 전달받은 데이터를 FlinkKafkaConsumerBase에 전달한다. KafkaConsumerThread: KafkaConsumer를 통해 데이터 Pull을 수행하고, 이 데이터를 KafkaFetcher에게 전달한다. Handover: KafkaConsuerThread와 KafkaFetcher간의 데이터 교환을 위한 Buffer Workflow FlinkKafkaConsumerBase의 run FlinkKafkaConsumerBase는 ParallelSourceFunction을 구현한 클래스로써, SourceFunction 관련 메서드들을 구현한다. 위의 사전 지식에서 서술한 것처럼 run 메서드에서 무한 루프를 돌며 Kafka에서 데이터를 Pull하고, 이 데이터를 SourceContext 객체의 collect 메서드를 호출하여 다음 Operator로 전달해야하는 의무를 가진다. public void run(SourceContext&lt;T&gt; sourceContext) throws Exception { ... 초기화 코드 this.kafkaFetcher = createFetcher( sourceContext, subscribedPartitionsToStartOffsets, watermarkStrategy, (StreamingRuntimeContext) getRuntimeContext(), offsetCommitMode, getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP), useMetrics); ... if (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) { kafkaFetcher.runFetchLoop(); } else { runWithPartitionDiscovery(); } } 위의 코드를 보면 Kafka로부터 데이터를 Pull하거나, 데이터를 가져오는 Loop를 유지하는 코드는 보이지 않는다. 이는 Flink 관련 기능과 Kafka 관련 기능의 역할 분리를 위한 것으로 판단되며, FlinkKafkaConsumerBase는 Flink 관련 기능, KafkaFetcher는 Kafka 관련 기능으로 역할을 부여받은 것으로 보인다. 때문에 FlinkKafkaConsumerBase는 실제 데이터 Fetch를 수행하는 KafkaFetcher 객체를 초기화하고 실행(runFetchLoop)하는 역할만을 담당한다. 후술하겠지만 KafkaFetcher의 runFetchLoop는 내부적으로 무한 루프가 수행되는 Blocking 메서드이기 때문에, Pipeline이 종료되기 전까지 FlinkKafkaConsumerBase의 run 메서드 또한 종료되지 않는다(Blocking 된다). KafkaFetcher의 runFetchLoop KafkaFetcher의 runFetchLoop 메서드 내부에 Loop 유지, Kafka로부터의 데이터 Pull, 다음 Operator에게 데이터 전달 등의 코드가 있을 것 같지만 그렇지 않다. KafkaFetcher는 Kafka로부터 데이터를 Pull하는 역할은 KafkaConsumerThread에게 위임하고, 자신은 Loop를 유지하며 KafkaConsumerThread로부터 전달받은 데이터를 다음 Operator에게 전달하는 역할만을 수행한다. runFetchLoop 메서드 public void runFetchLoop() throws Exception { try { // kick off the actual Kafka consumer consumerThread.start(); while (running) { final ConsumerRecords&lt;byte[], byte[]&gt; records = handover.pollNext(); for (KafkaTopicPartitionState&lt;T, TopicPartition&gt; partition : subscribedPartitionStates()) { List&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; partitionRecords = records.records(partition.getKafkaPartitionHandle()); partitionConsumerRecordsHandler(partitionRecords, partition); } } } finally { consumerThread.shutdown(); } try { consumerThread.join(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } runFetchLoop 메서드가 실행된 후 첫번째 행동은 KafkaConsumerThread를 실행하는 것이다. KafkaConsumerThread는 SourceFunction이 수행되는 Thread와 별도의 Thread로 실행되며, Background에서 Kafka Broker로부터 데이터를 Pull하고 이를 KafkaFetcher에게 전달하는 역할을 수행한다. KafkaFetcher는 Loop 내에서 Handover 객체의 pollNext 메서드를 수행한다. Handover는 KafkaConsumerThread가 KafkaFetcher에게 데이터를 전달하기 위한 Buffer 역할을 수행한다. Handover의 pollNext 메서드는 KafkaConsumerThread가 Kafka로부터 받아온 데이터가 있을 때 반환되는 Blocking 메서드이다. pollNext가 반환한 데이터는 partitionConsumerRecordsHandler 메서드의 매개변수로 전달되며, partitionConsumerRecordsHandler 내에서 Deserialize -&gt; Assign Timestamp -&gt; Emit 과정을 거쳐 다음 Operator에게 전달된다. KafkaConsumerThread의 역할 위의 KafkaFetcher는 runFetchLoop의 첫번째 라인에서 KafkaConsumerThread를 실행했다. 위에서도 서술했듯 KafkaConsumerThread는 SourceFunction과 별도의 Thread로 동작하며, 내부적으로 무한 루프를 돌며 Kafka Broker로부터 데이터를 Pull하고, 받아온 데이터를 Handover 객체를 통해 KafkaFetcher에게 전달한다. KafkaConsumerThread의 run 코드 일부 @Override public void run() { ... 생략 while (running) { if (records == null) { try { records = consumer.poll(pollTimeout); } catch (WakeupException we) { continue; } } try { handover.produce(records); records = null; } catch (Handover.WakeupException e) { // fall through the loop } ...생략 } 무한 루프가 존재하고, 내부에서 KafkaConsumer 객체의 poll 메서드를 호출하여 Kafka Broker로부터 데이터를 Pull 한 뒤, Handover의 produce 메서드를 호출하여 KafkaFetcher에게 데이터를 전달하는 것을 볼 수 있다. 재미있는 점은 데이터 Pull 과정에서의 예외 처리 방식이다. records 객체를 통해 Kafka로부터의 Poll에서의 예외와 Handover의 produce의 예외를 모두 처리하는데, 로직은 다음과 같다. records가 null인 경우 정상인 경우: 이전 Step에서 Kafka Broker로부터 정상적으로 데이터를 받아오고, 이를 Handover로 잘 전달하였다. 비정상인 경우: 이전 Step에서 Kafka Broker로부터 데이터를 받는데 실패했다.(첫번째 Catch 절에 걸려 continue됨) records가 null이 아닌 경우: 이전 Step에서 Handover로 데이터를 전달하지 못했다. 때문에 다시 Kafka Broker로부터 데이터를 받아올 필요 없이 전달에 실패한 데이터를 다시 Handover로 전달한다. Handover의 역할과 생산자-소비자 패턴 위에서 서술했듯 KafkaFetcher의 Loop와 KafkaConsumerThread의 Loop는 서로 다른 Thread에서 실행된다. Flink에서는 두 Thread간의 데이터 교환을 생산자-소비자 패턴을 통해 구현하였다. 여기서는 KafkaConsumerThread가 데이터를 생성하는 생산자(Producer) 역할을 하고, KafkaFetcher가 소비자(Consumer) 역할을 한다. 생성자-소비자 패턴에서 데이터 교환에는 보통 BlockingQueue가 사용되는데, 여기서는 Handover 객체가 BlockingQueue의 역할을 대체한다. Handover는 BlockingQueue의 역할을 수행하기 위해 내부적으로 2개의 변수를 활용한다. lock: Object 타입의 객체이며, KafkaConsumerThread와 KafkaFetcher가 서로의 동작이 끝나길 기다리거나(wait), 상대방에게 동작이 완료되었다고 알려주는(notifyAll) 역할을 수행한다. next: ConsumerRecords 타입의 객체이며, KafkaConsumerThread가 Kafka Broker로부터 Fetch 해온 데이터를 넣거나 KafkaFetcher가 다음 Operator에게 데이터를 전달할 때 조회하여 가져간다. 설명만으로는 이해가 어려우니 상황 별로 나누어 그림으로 설명 후 코드를 살펴보도록 한다. KafkaConsumerThread는 Kafka Broker로부터 Pull한 데이터를 Handover의 produce 호출을 통해 KafkaFetcher에게 전달하려 하고, KafkaFetcher는 Handover의 pollNext 호출을 통해 KafkaConsumerThread가 넣어놓은 데이터가 있다면 가져가려는 상황 1.KafkaConsumerThread가 Kafka Broker로부터 데이터를 가져와 Handover의 produce 호출. KafkaConsumerThread가 Handover의 lock을 획득하여 Critical Section에 진입. 2.KafkaFetcher가 1과 동시에 Handover의 pollNext 호출. KafkaFetcher는 Handover의 lock을 획득하려 하나 이미 KafkaConsumerThread가 lock을 선점하고 있기 때문에 Critical Section에 들어가지 못하고 Blocking 3.KafkaConsumerThread에서 Handover의 records에 Kafka Broker로부터 Pull했던 데이터를 할당. 작업이 완료되었으므로 lock을 반환하고 다음 데이터를 Fetch하러 감 4.3번째 Step에서 KafkaConsumerThread가 lock을 반환했기 때문에, KafkaFetcher는 lock 획득에 성공하여 Critical Section에 진입. 5.KafkaFetcher는 records에 데이터가 있는 것(KafkaConsumerThread가 넣어준)을 확인하고 이를 가져감. 또한 lock은 반환함 6.KafkaConsumerThread의 데이터 Pull이 오래 걸리는 상황에서 KafkaFetcher가 이전 Step의 모든 데이터 처리. 이에 KafkaFetcher는 다시 Handover의 pollNext 호출 7.KafkaFetcher는 Handover의 lock을 획득하고 records를 확인하지만 데이터가 존재하지 않음(null). 이에 lock의 wait을 호출하여 대기 상태로 진입 8.KafkaConsumerThread가 Kafka Broker로부터 데이터 Pull 완료. Handover의 produce 호출 9.7번째 Step에서 KafkaFetcher가 lock의 wait을 호출하며 lock 소유권을 반환했기 때문에 KafkaConsumerThread가 lock을 획득하며 Critical Section 진입. 10.KafkaConsumerThread는 자신이 Kafka Broker로부터 Pull한 데이터를 records에 할당. 이후 lock의 notifyAll을 호출하며 종료 11.KafkaFetcher는 10번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 5번째 Step 반복 12.KafkaConsumerThread가 1, 3번째 Step 수행 13.KafkaFetcher의 수행이 늦어져 KafkaConsumerThread는 다시 한번 1번째 Step 수행. 그러나 records에 아직 가져가지 않은 데이터가 존재하는 것을 확인하여 lock의 wait을 호출하여 대기 상태로 진입 14.KafkaFetcher의 데이터 처리가 완료되어 Handover의 pollNext 호출 15.13번째 Step에서 KafkaConsumerThread가 lock의 wait을 호출하며 lock의 소유권을 반환했기 때문에 KafkaFetcher가 lock을 획득하며 Critical Section 진입 16.KafkaFetcher는 5번 Step을 반복. 이 때 lock의 notifyAll을 호출 17.KafkaConsumerThread는 16번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 1, 3번째 Step 진행 위 과정을 코드로 살펴보면 아래와 같다. 생산자(KafkaConsumerThread)에서 사용하는 Handover의 produce public void produce(final ConsumerRecords&lt;byte[], byte[]&gt; element) throws InterruptedException, WakeupException, ClosedException { checkNotNull(element); synchronized (lock) { while (next != null &amp;&amp; !wakeupProducer) { lock.wait(); } wakeupProducer = false; // if there is still an element, we must have been woken up if (next != null) { throw new WakeupException(); } // if there is no error, then this is open and can accept this element else if (error == null) { next = element; lock.notifyAll(); } // an error marks this as closed for the producer else { throw new ClosedException(); } } } synchronized (lock) 구문을 통해 소비자(KafkaFetcher)와의 Race Condition을 없애고 있다. while(next != null &amp;&amp; !wakeupProducer) 구문을 통해 소비자가 데이터를 가져가지 않은 경우 lock을 반환하고 데이터를 가져갈 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 넣은 후(next = element)에는 혹시나 데이터를 넣기를 기다리고 있을 소비자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 소비자(KafkaFetcher)에서 사용하는 Handover의 pollNext public ConsumerRecords&lt;byte[], byte[]&gt; pollNext() throws Exception { synchronized (lock) { while (next == null &amp;&amp; error == null) { lock.wait(); } ConsumerRecords&lt;byte[], byte[]&gt; n = next; if (n != null) { next = null; lock.notifyAll(); return n; } else { ExceptionUtils.rethrowException(error, error.getMessage()); return ConsumerRecords.empty(); } } } synchronized (lock) 구문을 통해 생산자(KafkaConsumerThread)와의 Race Condition을 없애고 있다. while(next == null &amp;&amp; error == null) 구문을 통해 생산자가 데이터를 넣어주지 않은 경우 lock을 반환하고 데이터를 넣어줄 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 처리한 뒤(ConsumerRecords&lt;byte[], byte[]&gt; n = next)에는 혹시나 데이터를 소비하기를 기다리고 있을 생산자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 정리 웹이나 ETL 개발 등에는 잘 사용하지 않는 생산자-소비자 패턴과 Object의 wait이나 notifyAll의 자세한 사용법을 알아서 좋았다. 또한 Kafka &lt;-&gt; Flink 혹은 Flink 내의 Source &lt;-&gt; Operator 간의 속도가 다를 경우 어떤 관점에서는 마이크로 배치처럼 보일 수도 있겠구나 라는 생각도 들었다. 다음 글에는 Flink Kafka Consumer가 Checkpoint를 어떻게 처리하는지 알아볼 예정이다." /><meta property="og:description" content="개요 Flink의 FlinkKafkaConsumer와 같은 High Level API를 사용하다보면 궁금해지는 점들이 있다. Kafka의 Consumer는 Pull 방식으로 데이터를 가져오는데, Flink에서는 얼마 정도의 간격으로 Pull을 수행할까? Source와 다른 Operator를 Chaining하면 Kafka로부터 데이터를 Pull할 때 해당 Operator Chain이 Blocking 될까?(즉, I/O와 Computing이 합쳐져 있을까, 분리되어 있을까?에 대한 질문) Kafka Transaction이나 발생하는 Exception 은 어떻게 처리할까? 이번 글에서는 Flink에서 Kafka로부터 어떻게 데이터를 가져오고, 이를 어떻게 Pipeline에 전달하는지 알아보도록 한다. Flink의 Kafka Consumer 동작 방식 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); 위의 코드는 Flink의 공식문서에서 제공하는 Kafka Source 사용 방법이다. FlinkKafkaConsumer 클래스 하나만 노출하여 단순해보이지만, 내부적으로 많은 클래스가 Flink와 Kafka Consumer 연동에 관여하고 있기 때문에 전체 구조를 파악한 뒤, 어떤 흐름으로 동작하는지를 알아보도록 한다. 사전 지식 Flink의 SourceFunction Flink에 Data Source를 연동하기 위해서는 SourceFunction(ParallelSourceFunction)을 구현해야 한다. Kafka 또한 FlinkKafkaConsumer(의 부모 클래스인 FlinkKakfaConsumerBase)가 이를 구현하고 있다. SourceFunction을 상속받아 구현해야 하는 주요 메서드는 run과 cancel이 있다. run: Pipeline이 시작될 때 한번 호출되며, cancel이 호출되기 전까지 내부에서 Loop를 돌며 데이터를 만들어내야 한다. cancel: Pipeline이 종료될 때 호출되며, run에서 실행되고 있는 Loop를 멈춰야 하는 책임을 가진다. Flink에서 제공하는 run과 cancel의 예제 코드는 다음과 같다. run 메서드 public void run(SourceContext&lt;T&gt; ctx) { while (isRunning &amp;&amp; count &lt; 1000) { synchronized (ctx.getCheckpointLock()) { ctx.collect(count); count++; } } } Loop가 계속 돌 수 있는지 체크하는 isRunning을 체크하며 Loop를 돌고 있고, run 메서드 호출 시 매개변수로 전달받은 SourceContext 객체인 ctx의 collect 메서드를 통해 다음 Operator로 데이터를 전달하는 것을 확인할 수 있다. 또한 CheckPoint가 수행될 때(synchronized)는 데이터를 만들어내지 않는 것을 확인할 수 있다.(추후에 확인해봐야겠지만 이 때 Checkpoint Barrier가 전달되는 것으로 예상된다) cancel 메서드 public void cancel() { isRunning = false; } run 메서드의 Loop에서 사용하는 isRunning 값을 false로 변경하므로써, SourceFunction의 동작을 멈추는 것을 확인할 수 있다. Architecture Overview Flink의 Kafka Consumer를 구성하는 전반적인 클래스 다이어그램이다. 주요 클래스와 그 역할은 아래와 같다. FlinkKafkaConsumerBase: Flink의 SourceFunction을 구현하며, 실제 데이터를 KafkaFetcher 객체를 통해 전달받는다. KafkaFetcher: KafkaConsumerThread에서 전달받은 데이터를 FlinkKafkaConsumerBase에 전달한다. KafkaConsumerThread: KafkaConsumer를 통해 데이터 Pull을 수행하고, 이 데이터를 KafkaFetcher에게 전달한다. Handover: KafkaConsuerThread와 KafkaFetcher간의 데이터 교환을 위한 Buffer Workflow FlinkKafkaConsumerBase의 run FlinkKafkaConsumerBase는 ParallelSourceFunction을 구현한 클래스로써, SourceFunction 관련 메서드들을 구현한다. 위의 사전 지식에서 서술한 것처럼 run 메서드에서 무한 루프를 돌며 Kafka에서 데이터를 Pull하고, 이 데이터를 SourceContext 객체의 collect 메서드를 호출하여 다음 Operator로 전달해야하는 의무를 가진다. public void run(SourceContext&lt;T&gt; sourceContext) throws Exception { ... 초기화 코드 this.kafkaFetcher = createFetcher( sourceContext, subscribedPartitionsToStartOffsets, watermarkStrategy, (StreamingRuntimeContext) getRuntimeContext(), offsetCommitMode, getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP), useMetrics); ... if (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) { kafkaFetcher.runFetchLoop(); } else { runWithPartitionDiscovery(); } } 위의 코드를 보면 Kafka로부터 데이터를 Pull하거나, 데이터를 가져오는 Loop를 유지하는 코드는 보이지 않는다. 이는 Flink 관련 기능과 Kafka 관련 기능의 역할 분리를 위한 것으로 판단되며, FlinkKafkaConsumerBase는 Flink 관련 기능, KafkaFetcher는 Kafka 관련 기능으로 역할을 부여받은 것으로 보인다. 때문에 FlinkKafkaConsumerBase는 실제 데이터 Fetch를 수행하는 KafkaFetcher 객체를 초기화하고 실행(runFetchLoop)하는 역할만을 담당한다. 후술하겠지만 KafkaFetcher의 runFetchLoop는 내부적으로 무한 루프가 수행되는 Blocking 메서드이기 때문에, Pipeline이 종료되기 전까지 FlinkKafkaConsumerBase의 run 메서드 또한 종료되지 않는다(Blocking 된다). KafkaFetcher의 runFetchLoop KafkaFetcher의 runFetchLoop 메서드 내부에 Loop 유지, Kafka로부터의 데이터 Pull, 다음 Operator에게 데이터 전달 등의 코드가 있을 것 같지만 그렇지 않다. KafkaFetcher는 Kafka로부터 데이터를 Pull하는 역할은 KafkaConsumerThread에게 위임하고, 자신은 Loop를 유지하며 KafkaConsumerThread로부터 전달받은 데이터를 다음 Operator에게 전달하는 역할만을 수행한다. runFetchLoop 메서드 public void runFetchLoop() throws Exception { try { // kick off the actual Kafka consumer consumerThread.start(); while (running) { final ConsumerRecords&lt;byte[], byte[]&gt; records = handover.pollNext(); for (KafkaTopicPartitionState&lt;T, TopicPartition&gt; partition : subscribedPartitionStates()) { List&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; partitionRecords = records.records(partition.getKafkaPartitionHandle()); partitionConsumerRecordsHandler(partitionRecords, partition); } } } finally { consumerThread.shutdown(); } try { consumerThread.join(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } runFetchLoop 메서드가 실행된 후 첫번째 행동은 KafkaConsumerThread를 실행하는 것이다. KafkaConsumerThread는 SourceFunction이 수행되는 Thread와 별도의 Thread로 실행되며, Background에서 Kafka Broker로부터 데이터를 Pull하고 이를 KafkaFetcher에게 전달하는 역할을 수행한다. KafkaFetcher는 Loop 내에서 Handover 객체의 pollNext 메서드를 수행한다. Handover는 KafkaConsumerThread가 KafkaFetcher에게 데이터를 전달하기 위한 Buffer 역할을 수행한다. Handover의 pollNext 메서드는 KafkaConsumerThread가 Kafka로부터 받아온 데이터가 있을 때 반환되는 Blocking 메서드이다. pollNext가 반환한 데이터는 partitionConsumerRecordsHandler 메서드의 매개변수로 전달되며, partitionConsumerRecordsHandler 내에서 Deserialize -&gt; Assign Timestamp -&gt; Emit 과정을 거쳐 다음 Operator에게 전달된다. KafkaConsumerThread의 역할 위의 KafkaFetcher는 runFetchLoop의 첫번째 라인에서 KafkaConsumerThread를 실행했다. 위에서도 서술했듯 KafkaConsumerThread는 SourceFunction과 별도의 Thread로 동작하며, 내부적으로 무한 루프를 돌며 Kafka Broker로부터 데이터를 Pull하고, 받아온 데이터를 Handover 객체를 통해 KafkaFetcher에게 전달한다. KafkaConsumerThread의 run 코드 일부 @Override public void run() { ... 생략 while (running) { if (records == null) { try { records = consumer.poll(pollTimeout); } catch (WakeupException we) { continue; } } try { handover.produce(records); records = null; } catch (Handover.WakeupException e) { // fall through the loop } ...생략 } 무한 루프가 존재하고, 내부에서 KafkaConsumer 객체의 poll 메서드를 호출하여 Kafka Broker로부터 데이터를 Pull 한 뒤, Handover의 produce 메서드를 호출하여 KafkaFetcher에게 데이터를 전달하는 것을 볼 수 있다. 재미있는 점은 데이터 Pull 과정에서의 예외 처리 방식이다. records 객체를 통해 Kafka로부터의 Poll에서의 예외와 Handover의 produce의 예외를 모두 처리하는데, 로직은 다음과 같다. records가 null인 경우 정상인 경우: 이전 Step에서 Kafka Broker로부터 정상적으로 데이터를 받아오고, 이를 Handover로 잘 전달하였다. 비정상인 경우: 이전 Step에서 Kafka Broker로부터 데이터를 받는데 실패했다.(첫번째 Catch 절에 걸려 continue됨) records가 null이 아닌 경우: 이전 Step에서 Handover로 데이터를 전달하지 못했다. 때문에 다시 Kafka Broker로부터 데이터를 받아올 필요 없이 전달에 실패한 데이터를 다시 Handover로 전달한다. Handover의 역할과 생산자-소비자 패턴 위에서 서술했듯 KafkaFetcher의 Loop와 KafkaConsumerThread의 Loop는 서로 다른 Thread에서 실행된다. Flink에서는 두 Thread간의 데이터 교환을 생산자-소비자 패턴을 통해 구현하였다. 여기서는 KafkaConsumerThread가 데이터를 생성하는 생산자(Producer) 역할을 하고, KafkaFetcher가 소비자(Consumer) 역할을 한다. 생성자-소비자 패턴에서 데이터 교환에는 보통 BlockingQueue가 사용되는데, 여기서는 Handover 객체가 BlockingQueue의 역할을 대체한다. Handover는 BlockingQueue의 역할을 수행하기 위해 내부적으로 2개의 변수를 활용한다. lock: Object 타입의 객체이며, KafkaConsumerThread와 KafkaFetcher가 서로의 동작이 끝나길 기다리거나(wait), 상대방에게 동작이 완료되었다고 알려주는(notifyAll) 역할을 수행한다. next: ConsumerRecords 타입의 객체이며, KafkaConsumerThread가 Kafka Broker로부터 Fetch 해온 데이터를 넣거나 KafkaFetcher가 다음 Operator에게 데이터를 전달할 때 조회하여 가져간다. 설명만으로는 이해가 어려우니 상황 별로 나누어 그림으로 설명 후 코드를 살펴보도록 한다. KafkaConsumerThread는 Kafka Broker로부터 Pull한 데이터를 Handover의 produce 호출을 통해 KafkaFetcher에게 전달하려 하고, KafkaFetcher는 Handover의 pollNext 호출을 통해 KafkaConsumerThread가 넣어놓은 데이터가 있다면 가져가려는 상황 1.KafkaConsumerThread가 Kafka Broker로부터 데이터를 가져와 Handover의 produce 호출. KafkaConsumerThread가 Handover의 lock을 획득하여 Critical Section에 진입. 2.KafkaFetcher가 1과 동시에 Handover의 pollNext 호출. KafkaFetcher는 Handover의 lock을 획득하려 하나 이미 KafkaConsumerThread가 lock을 선점하고 있기 때문에 Critical Section에 들어가지 못하고 Blocking 3.KafkaConsumerThread에서 Handover의 records에 Kafka Broker로부터 Pull했던 데이터를 할당. 작업이 완료되었으므로 lock을 반환하고 다음 데이터를 Fetch하러 감 4.3번째 Step에서 KafkaConsumerThread가 lock을 반환했기 때문에, KafkaFetcher는 lock 획득에 성공하여 Critical Section에 진입. 5.KafkaFetcher는 records에 데이터가 있는 것(KafkaConsumerThread가 넣어준)을 확인하고 이를 가져감. 또한 lock은 반환함 6.KafkaConsumerThread의 데이터 Pull이 오래 걸리는 상황에서 KafkaFetcher가 이전 Step의 모든 데이터 처리. 이에 KafkaFetcher는 다시 Handover의 pollNext 호출 7.KafkaFetcher는 Handover의 lock을 획득하고 records를 확인하지만 데이터가 존재하지 않음(null). 이에 lock의 wait을 호출하여 대기 상태로 진입 8.KafkaConsumerThread가 Kafka Broker로부터 데이터 Pull 완료. Handover의 produce 호출 9.7번째 Step에서 KafkaFetcher가 lock의 wait을 호출하며 lock 소유권을 반환했기 때문에 KafkaConsumerThread가 lock을 획득하며 Critical Section 진입. 10.KafkaConsumerThread는 자신이 Kafka Broker로부터 Pull한 데이터를 records에 할당. 이후 lock의 notifyAll을 호출하며 종료 11.KafkaFetcher는 10번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 5번째 Step 반복 12.KafkaConsumerThread가 1, 3번째 Step 수행 13.KafkaFetcher의 수행이 늦어져 KafkaConsumerThread는 다시 한번 1번째 Step 수행. 그러나 records에 아직 가져가지 않은 데이터가 존재하는 것을 확인하여 lock의 wait을 호출하여 대기 상태로 진입 14.KafkaFetcher의 데이터 처리가 완료되어 Handover의 pollNext 호출 15.13번째 Step에서 KafkaConsumerThread가 lock의 wait을 호출하며 lock의 소유권을 반환했기 때문에 KafkaFetcher가 lock을 획득하며 Critical Section 진입 16.KafkaFetcher는 5번 Step을 반복. 이 때 lock의 notifyAll을 호출 17.KafkaConsumerThread는 16번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 1, 3번째 Step 진행 위 과정을 코드로 살펴보면 아래와 같다. 생산자(KafkaConsumerThread)에서 사용하는 Handover의 produce public void produce(final ConsumerRecords&lt;byte[], byte[]&gt; element) throws InterruptedException, WakeupException, ClosedException { checkNotNull(element); synchronized (lock) { while (next != null &amp;&amp; !wakeupProducer) { lock.wait(); } wakeupProducer = false; // if there is still an element, we must have been woken up if (next != null) { throw new WakeupException(); } // if there is no error, then this is open and can accept this element else if (error == null) { next = element; lock.notifyAll(); } // an error marks this as closed for the producer else { throw new ClosedException(); } } } synchronized (lock) 구문을 통해 소비자(KafkaFetcher)와의 Race Condition을 없애고 있다. while(next != null &amp;&amp; !wakeupProducer) 구문을 통해 소비자가 데이터를 가져가지 않은 경우 lock을 반환하고 데이터를 가져갈 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 넣은 후(next = element)에는 혹시나 데이터를 넣기를 기다리고 있을 소비자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 소비자(KafkaFetcher)에서 사용하는 Handover의 pollNext public ConsumerRecords&lt;byte[], byte[]&gt; pollNext() throws Exception { synchronized (lock) { while (next == null &amp;&amp; error == null) { lock.wait(); } ConsumerRecords&lt;byte[], byte[]&gt; n = next; if (n != null) { next = null; lock.notifyAll(); return n; } else { ExceptionUtils.rethrowException(error, error.getMessage()); return ConsumerRecords.empty(); } } } synchronized (lock) 구문을 통해 생산자(KafkaConsumerThread)와의 Race Condition을 없애고 있다. while(next == null &amp;&amp; error == null) 구문을 통해 생산자가 데이터를 넣어주지 않은 경우 lock을 반환하고 데이터를 넣어줄 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 처리한 뒤(ConsumerRecords&lt;byte[], byte[]&gt; n = next)에는 혹시나 데이터를 소비하기를 기다리고 있을 생산자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 정리 웹이나 ETL 개발 등에는 잘 사용하지 않는 생산자-소비자 패턴과 Object의 wait이나 notifyAll의 자세한 사용법을 알아서 좋았다. 또한 Kafka &lt;-&gt; Flink 혹은 Flink 내의 Source &lt;-&gt; Operator 간의 속도가 다를 경우 어떤 관점에서는 마이크로 배치처럼 보일 수도 있겠구나 라는 생각도 들었다. 다음 글에는 Flink Kafka Consumer가 Checkpoint를 어떻게 처리하는지 알아볼 예정이다." /><link rel="canonical" href="https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/" /><meta property="og:url" content="https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/" /><meta property="og:site_name" content="leeyh0216’s devlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-04-15T00:50:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Flink Concept - Flink의 Kafka Consumer 동작 방식(1)" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@leeyh0216" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"leeyh0216"},"description":"개요 Flink의 FlinkKafkaConsumer와 같은 High Level API를 사용하다보면 궁금해지는 점들이 있다. Kafka의 Consumer는 Pull 방식으로 데이터를 가져오는데, Flink에서는 얼마 정도의 간격으로 Pull을 수행할까? Source와 다른 Operator를 Chaining하면 Kafka로부터 데이터를 Pull할 때 해당 Operator Chain이 Blocking 될까?(즉, I/O와 Computing이 합쳐져 있을까, 분리되어 있을까?에 대한 질문) Kafka Transaction이나 발생하는 Exception 은 어떻게 처리할까? 이번 글에서는 Flink에서 Kafka로부터 어떻게 데이터를 가져오고, 이를 어떻게 Pipeline에 전달하는지 알아보도록 한다. Flink의 Kafka Consumer 동작 방식 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); 위의 코드는 Flink의 공식문서에서 제공하는 Kafka Source 사용 방법이다. FlinkKafkaConsumer 클래스 하나만 노출하여 단순해보이지만, 내부적으로 많은 클래스가 Flink와 Kafka Consumer 연동에 관여하고 있기 때문에 전체 구조를 파악한 뒤, 어떤 흐름으로 동작하는지를 알아보도록 한다. 사전 지식 Flink의 SourceFunction Flink에 Data Source를 연동하기 위해서는 SourceFunction(ParallelSourceFunction)을 구현해야 한다. Kafka 또한 FlinkKafkaConsumer(의 부모 클래스인 FlinkKakfaConsumerBase)가 이를 구현하고 있다. SourceFunction을 상속받아 구현해야 하는 주요 메서드는 run과 cancel이 있다. run: Pipeline이 시작될 때 한번 호출되며, cancel이 호출되기 전까지 내부에서 Loop를 돌며 데이터를 만들어내야 한다. cancel: Pipeline이 종료될 때 호출되며, run에서 실행되고 있는 Loop를 멈춰야 하는 책임을 가진다. Flink에서 제공하는 run과 cancel의 예제 코드는 다음과 같다. run 메서드 public void run(SourceContext&lt;T&gt; ctx) { while (isRunning &amp;&amp; count &lt; 1000) { synchronized (ctx.getCheckpointLock()) { ctx.collect(count); count++; } } } Loop가 계속 돌 수 있는지 체크하는 isRunning을 체크하며 Loop를 돌고 있고, run 메서드 호출 시 매개변수로 전달받은 SourceContext 객체인 ctx의 collect 메서드를 통해 다음 Operator로 데이터를 전달하는 것을 확인할 수 있다. 또한 CheckPoint가 수행될 때(synchronized)는 데이터를 만들어내지 않는 것을 확인할 수 있다.(추후에 확인해봐야겠지만 이 때 Checkpoint Barrier가 전달되는 것으로 예상된다) cancel 메서드 public void cancel() { isRunning = false; } run 메서드의 Loop에서 사용하는 isRunning 값을 false로 변경하므로써, SourceFunction의 동작을 멈추는 것을 확인할 수 있다. Architecture Overview Flink의 Kafka Consumer를 구성하는 전반적인 클래스 다이어그램이다. 주요 클래스와 그 역할은 아래와 같다. FlinkKafkaConsumerBase: Flink의 SourceFunction을 구현하며, 실제 데이터를 KafkaFetcher 객체를 통해 전달받는다. KafkaFetcher: KafkaConsumerThread에서 전달받은 데이터를 FlinkKafkaConsumerBase에 전달한다. KafkaConsumerThread: KafkaConsumer를 통해 데이터 Pull을 수행하고, 이 데이터를 KafkaFetcher에게 전달한다. Handover: KafkaConsuerThread와 KafkaFetcher간의 데이터 교환을 위한 Buffer Workflow FlinkKafkaConsumerBase의 run FlinkKafkaConsumerBase는 ParallelSourceFunction을 구현한 클래스로써, SourceFunction 관련 메서드들을 구현한다. 위의 사전 지식에서 서술한 것처럼 run 메서드에서 무한 루프를 돌며 Kafka에서 데이터를 Pull하고, 이 데이터를 SourceContext 객체의 collect 메서드를 호출하여 다음 Operator로 전달해야하는 의무를 가진다. public void run(SourceContext&lt;T&gt; sourceContext) throws Exception { ... 초기화 코드 this.kafkaFetcher = createFetcher( sourceContext, subscribedPartitionsToStartOffsets, watermarkStrategy, (StreamingRuntimeContext) getRuntimeContext(), offsetCommitMode, getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP), useMetrics); ... if (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) { kafkaFetcher.runFetchLoop(); } else { runWithPartitionDiscovery(); } } 위의 코드를 보면 Kafka로부터 데이터를 Pull하거나, 데이터를 가져오는 Loop를 유지하는 코드는 보이지 않는다. 이는 Flink 관련 기능과 Kafka 관련 기능의 역할 분리를 위한 것으로 판단되며, FlinkKafkaConsumerBase는 Flink 관련 기능, KafkaFetcher는 Kafka 관련 기능으로 역할을 부여받은 것으로 보인다. 때문에 FlinkKafkaConsumerBase는 실제 데이터 Fetch를 수행하는 KafkaFetcher 객체를 초기화하고 실행(runFetchLoop)하는 역할만을 담당한다. 후술하겠지만 KafkaFetcher의 runFetchLoop는 내부적으로 무한 루프가 수행되는 Blocking 메서드이기 때문에, Pipeline이 종료되기 전까지 FlinkKafkaConsumerBase의 run 메서드 또한 종료되지 않는다(Blocking 된다). KafkaFetcher의 runFetchLoop KafkaFetcher의 runFetchLoop 메서드 내부에 Loop 유지, Kafka로부터의 데이터 Pull, 다음 Operator에게 데이터 전달 등의 코드가 있을 것 같지만 그렇지 않다. KafkaFetcher는 Kafka로부터 데이터를 Pull하는 역할은 KafkaConsumerThread에게 위임하고, 자신은 Loop를 유지하며 KafkaConsumerThread로부터 전달받은 데이터를 다음 Operator에게 전달하는 역할만을 수행한다. runFetchLoop 메서드 public void runFetchLoop() throws Exception { try { // kick off the actual Kafka consumer consumerThread.start(); while (running) { final ConsumerRecords&lt;byte[], byte[]&gt; records = handover.pollNext(); for (KafkaTopicPartitionState&lt;T, TopicPartition&gt; partition : subscribedPartitionStates()) { List&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; partitionRecords = records.records(partition.getKafkaPartitionHandle()); partitionConsumerRecordsHandler(partitionRecords, partition); } } } finally { consumerThread.shutdown(); } try { consumerThread.join(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } runFetchLoop 메서드가 실행된 후 첫번째 행동은 KafkaConsumerThread를 실행하는 것이다. KafkaConsumerThread는 SourceFunction이 수행되는 Thread와 별도의 Thread로 실행되며, Background에서 Kafka Broker로부터 데이터를 Pull하고 이를 KafkaFetcher에게 전달하는 역할을 수행한다. KafkaFetcher는 Loop 내에서 Handover 객체의 pollNext 메서드를 수행한다. Handover는 KafkaConsumerThread가 KafkaFetcher에게 데이터를 전달하기 위한 Buffer 역할을 수행한다. Handover의 pollNext 메서드는 KafkaConsumerThread가 Kafka로부터 받아온 데이터가 있을 때 반환되는 Blocking 메서드이다. pollNext가 반환한 데이터는 partitionConsumerRecordsHandler 메서드의 매개변수로 전달되며, partitionConsumerRecordsHandler 내에서 Deserialize -&gt; Assign Timestamp -&gt; Emit 과정을 거쳐 다음 Operator에게 전달된다. KafkaConsumerThread의 역할 위의 KafkaFetcher는 runFetchLoop의 첫번째 라인에서 KafkaConsumerThread를 실행했다. 위에서도 서술했듯 KafkaConsumerThread는 SourceFunction과 별도의 Thread로 동작하며, 내부적으로 무한 루프를 돌며 Kafka Broker로부터 데이터를 Pull하고, 받아온 데이터를 Handover 객체를 통해 KafkaFetcher에게 전달한다. KafkaConsumerThread의 run 코드 일부 @Override public void run() { ... 생략 while (running) { if (records == null) { try { records = consumer.poll(pollTimeout); } catch (WakeupException we) { continue; } } try { handover.produce(records); records = null; } catch (Handover.WakeupException e) { // fall through the loop } ...생략 } 무한 루프가 존재하고, 내부에서 KafkaConsumer 객체의 poll 메서드를 호출하여 Kafka Broker로부터 데이터를 Pull 한 뒤, Handover의 produce 메서드를 호출하여 KafkaFetcher에게 데이터를 전달하는 것을 볼 수 있다. 재미있는 점은 데이터 Pull 과정에서의 예외 처리 방식이다. records 객체를 통해 Kafka로부터의 Poll에서의 예외와 Handover의 produce의 예외를 모두 처리하는데, 로직은 다음과 같다. records가 null인 경우 정상인 경우: 이전 Step에서 Kafka Broker로부터 정상적으로 데이터를 받아오고, 이를 Handover로 잘 전달하였다. 비정상인 경우: 이전 Step에서 Kafka Broker로부터 데이터를 받는데 실패했다.(첫번째 Catch 절에 걸려 continue됨) records가 null이 아닌 경우: 이전 Step에서 Handover로 데이터를 전달하지 못했다. 때문에 다시 Kafka Broker로부터 데이터를 받아올 필요 없이 전달에 실패한 데이터를 다시 Handover로 전달한다. Handover의 역할과 생산자-소비자 패턴 위에서 서술했듯 KafkaFetcher의 Loop와 KafkaConsumerThread의 Loop는 서로 다른 Thread에서 실행된다. Flink에서는 두 Thread간의 데이터 교환을 생산자-소비자 패턴을 통해 구현하였다. 여기서는 KafkaConsumerThread가 데이터를 생성하는 생산자(Producer) 역할을 하고, KafkaFetcher가 소비자(Consumer) 역할을 한다. 생성자-소비자 패턴에서 데이터 교환에는 보통 BlockingQueue가 사용되는데, 여기서는 Handover 객체가 BlockingQueue의 역할을 대체한다. Handover는 BlockingQueue의 역할을 수행하기 위해 내부적으로 2개의 변수를 활용한다. lock: Object 타입의 객체이며, KafkaConsumerThread와 KafkaFetcher가 서로의 동작이 끝나길 기다리거나(wait), 상대방에게 동작이 완료되었다고 알려주는(notifyAll) 역할을 수행한다. next: ConsumerRecords 타입의 객체이며, KafkaConsumerThread가 Kafka Broker로부터 Fetch 해온 데이터를 넣거나 KafkaFetcher가 다음 Operator에게 데이터를 전달할 때 조회하여 가져간다. 설명만으로는 이해가 어려우니 상황 별로 나누어 그림으로 설명 후 코드를 살펴보도록 한다. KafkaConsumerThread는 Kafka Broker로부터 Pull한 데이터를 Handover의 produce 호출을 통해 KafkaFetcher에게 전달하려 하고, KafkaFetcher는 Handover의 pollNext 호출을 통해 KafkaConsumerThread가 넣어놓은 데이터가 있다면 가져가려는 상황 1.KafkaConsumerThread가 Kafka Broker로부터 데이터를 가져와 Handover의 produce 호출. KafkaConsumerThread가 Handover의 lock을 획득하여 Critical Section에 진입. 2.KafkaFetcher가 1과 동시에 Handover의 pollNext 호출. KafkaFetcher는 Handover의 lock을 획득하려 하나 이미 KafkaConsumerThread가 lock을 선점하고 있기 때문에 Critical Section에 들어가지 못하고 Blocking 3.KafkaConsumerThread에서 Handover의 records에 Kafka Broker로부터 Pull했던 데이터를 할당. 작업이 완료되었으므로 lock을 반환하고 다음 데이터를 Fetch하러 감 4.3번째 Step에서 KafkaConsumerThread가 lock을 반환했기 때문에, KafkaFetcher는 lock 획득에 성공하여 Critical Section에 진입. 5.KafkaFetcher는 records에 데이터가 있는 것(KafkaConsumerThread가 넣어준)을 확인하고 이를 가져감. 또한 lock은 반환함 6.KafkaConsumerThread의 데이터 Pull이 오래 걸리는 상황에서 KafkaFetcher가 이전 Step의 모든 데이터 처리. 이에 KafkaFetcher는 다시 Handover의 pollNext 호출 7.KafkaFetcher는 Handover의 lock을 획득하고 records를 확인하지만 데이터가 존재하지 않음(null). 이에 lock의 wait을 호출하여 대기 상태로 진입 8.KafkaConsumerThread가 Kafka Broker로부터 데이터 Pull 완료. Handover의 produce 호출 9.7번째 Step에서 KafkaFetcher가 lock의 wait을 호출하며 lock 소유권을 반환했기 때문에 KafkaConsumerThread가 lock을 획득하며 Critical Section 진입. 10.KafkaConsumerThread는 자신이 Kafka Broker로부터 Pull한 데이터를 records에 할당. 이후 lock의 notifyAll을 호출하며 종료 11.KafkaFetcher는 10번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 5번째 Step 반복 12.KafkaConsumerThread가 1, 3번째 Step 수행 13.KafkaFetcher의 수행이 늦어져 KafkaConsumerThread는 다시 한번 1번째 Step 수행. 그러나 records에 아직 가져가지 않은 데이터가 존재하는 것을 확인하여 lock의 wait을 호출하여 대기 상태로 진입 14.KafkaFetcher의 데이터 처리가 완료되어 Handover의 pollNext 호출 15.13번째 Step에서 KafkaConsumerThread가 lock의 wait을 호출하며 lock의 소유권을 반환했기 때문에 KafkaFetcher가 lock을 획득하며 Critical Section 진입 16.KafkaFetcher는 5번 Step을 반복. 이 때 lock의 notifyAll을 호출 17.KafkaConsumerThread는 16번째 Step에서 호출된 notifyAll에 의해 WAIT에서 RUNNING 상태로 진입. 1, 3번째 Step 진행 위 과정을 코드로 살펴보면 아래와 같다. 생산자(KafkaConsumerThread)에서 사용하는 Handover의 produce public void produce(final ConsumerRecords&lt;byte[], byte[]&gt; element) throws InterruptedException, WakeupException, ClosedException { checkNotNull(element); synchronized (lock) { while (next != null &amp;&amp; !wakeupProducer) { lock.wait(); } wakeupProducer = false; // if there is still an element, we must have been woken up if (next != null) { throw new WakeupException(); } // if there is no error, then this is open and can accept this element else if (error == null) { next = element; lock.notifyAll(); } // an error marks this as closed for the producer else { throw new ClosedException(); } } } synchronized (lock) 구문을 통해 소비자(KafkaFetcher)와의 Race Condition을 없애고 있다. while(next != null &amp;&amp; !wakeupProducer) 구문을 통해 소비자가 데이터를 가져가지 않은 경우 lock을 반환하고 데이터를 가져갈 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 넣은 후(next = element)에는 혹시나 데이터를 넣기를 기다리고 있을 소비자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 소비자(KafkaFetcher)에서 사용하는 Handover의 pollNext public ConsumerRecords&lt;byte[], byte[]&gt; pollNext() throws Exception { synchronized (lock) { while (next == null &amp;&amp; error == null) { lock.wait(); } ConsumerRecords&lt;byte[], byte[]&gt; n = next; if (n != null) { next = null; lock.notifyAll(); return n; } else { ExceptionUtils.rethrowException(error, error.getMessage()); return ConsumerRecords.empty(); } } } synchronized (lock) 구문을 통해 생산자(KafkaConsumerThread)와의 Race Condition을 없애고 있다. while(next == null &amp;&amp; error == null) 구문을 통해 생산자가 데이터를 넣어주지 않은 경우 lock을 반환하고 데이터를 넣어줄 때까지 대기(lock.wait)하는 것을 볼 수 있다. 데이터를 처리한 뒤(ConsumerRecords&lt;byte[], byte[]&gt; n = next)에는 혹시나 데이터를 소비하기를 기다리고 있을 생산자를 깨우기 위해 lock.notifyAll을 호출하는 것을 볼 수 있다. 정리 웹이나 ETL 개발 등에는 잘 사용하지 않는 생산자-소비자 패턴과 Object의 wait이나 notifyAll의 자세한 사용법을 알아서 좋았다. 또한 Kafka &lt;-&gt; Flink 혹은 Flink 내의 Source &lt;-&gt; Operator 간의 속도가 다를 경우 어떤 관점에서는 마이크로 배치처럼 보일 수도 있겠구나 라는 생각도 들었다. 다음 글에는 Flink Kafka Consumer가 Checkpoint를 어떻게 처리하는지 알아볼 예정이다.","url":"https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/","@type":"BlogPosting","headline":"Flink Concept - Flink의 Kafka Consumer 동작 방식(1)","dateModified":"2021-04-15T00:50:00+09:00","datePublished":"2021-04-15T00:50:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/"},"@context":"https://schema.org"}</script><title>Flink Concept - Flink의 Kafka Consumer 동작 방식(1) | leeyh0216's devlog</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script async src="/assets/js/dist/pvreport.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-129061352-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-129061352-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">leeyh0216's devlog</a></div><div class="site-subtitle font-italic">개발/일상 블로그</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/leeyh0216" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EC%9A%A9%ED%99%98-%EC%9D%B4-84222a119/" aria-label="linkedin" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['leeyh0216','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Flink Concept - Flink의 Kafka Consumer 동작 방식(1)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Flink Concept - Flink의 Kafka Consumer 동작 방식(1)</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> leeyh0216 </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Apr 15, 2021, 12:50 AM +0900" prep="on" > Apr 15, 2021 <i class="unloaded">2021-04-15T00:50:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3555 words">19 min</span> <span id="pv" class="pageviews"><i class="fas fa-spinner fa-spin fa-fw"></i></span></div></div><div class="post-content"><h1 id="개요">개요</h1><p>Flink의 <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumer</code>와 같은 High Level API를 사용하다보면 궁금해지는 점들이 있다.</p><ul><li>Kafka의 Consumer는 Pull 방식으로 데이터를 가져오는데, Flink에서는 얼마 정도의 간격으로 Pull을 수행할까?<li>Source와 다른 Operator를 Chaining하면 Kafka로부터 데이터를 Pull할 때 해당 Operator Chain이 Blocking 될까?(즉, I/O와 Computing이 합쳐져 있을까, 분리되어 있을까?에 대한 질문)<li>Kafka Transaction이나 발생하는 Exception 은 어떻게 처리할까?</ul><p>이번 글에서는 Flink에서 Kafka로부터 어떻게 데이터를 가져오고, 이를 어떻게 Pipeline에 전달하는지 알아보도록 한다.</p><h1 id="flink의-kafka-consumer-동작-방식">Flink의 Kafka Consumer 동작 방식</h1><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span>
	<span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">));</span></code></pre></figure><p>위의 코드는 Flink의 공식문서에서 제공하는 Kafka Source 사용 방법이다.</p><p><code class="language-plaintext highlighter-rouge">FlinkKafkaConsumer</code> 클래스 하나만 노출하여 단순해보이지만, 내부적으로 많은 클래스가 Flink와 Kafka Consumer 연동에 관여하고 있기 때문에 전체 구조를 파악한 뒤, 어떤 흐름으로 동작하는지를 알아보도록 한다.</p><h2 id="사전-지식">사전 지식</h2><h3 id="flink의-sourcefunction">Flink의 <code class="language-plaintext highlighter-rouge">SourceFunction</code></h3><p>Flink에 Data Source를 연동하기 위해서는 <code class="language-plaintext highlighter-rouge">SourceFunction</code>(<code class="language-plaintext highlighter-rouge">ParallelSourceFunction</code>)을 구현해야 한다. Kafka 또한 <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumer</code>(의 부모 클래스인 <code class="language-plaintext highlighter-rouge">FlinkKakfaConsumerBase</code>)가 이를 구현하고 있다.</p><p><code class="language-plaintext highlighter-rouge">SourceFunction</code>을 상속받아 구현해야 하는 주요 메서드는 <code class="language-plaintext highlighter-rouge">run</code>과 <code class="language-plaintext highlighter-rouge">cancel</code>이 있다.</p><ul><li><code class="language-plaintext highlighter-rouge">run</code>: Pipeline이 시작될 때 한번 호출되며, <code class="language-plaintext highlighter-rouge">cancel</code>이 호출되기 전까지 내부에서 Loop를 돌며 데이터를 만들어내야 한다.<li><code class="language-plaintext highlighter-rouge">cancel</code>: Pipeline이 종료될 때 호출되며, <code class="language-plaintext highlighter-rouge">run</code>에서 실행되고 있는 Loop를 멈춰야 하는 책임을 가진다.</ul><p>Flink에서 제공하는 <code class="language-plaintext highlighter-rouge">run</code>과 <code class="language-plaintext highlighter-rouge">cancel</code>의 예제 코드는 다음과 같다.</p><p><strong><code class="language-plaintext highlighter-rouge">run</code> 메서드</strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="nc">SourceContext</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">ctx</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">while</span> <span class="o">(</span><span class="n">isRunning</span> <span class="o">&amp;&amp;</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="o">)</span> <span class="o">{</span>
    <span class="kd">synchronized</span> <span class="o">(</span><span class="n">ctx</span><span class="o">.</span><span class="na">getCheckpointLock</span><span class="o">())</span> <span class="o">{</span>
      <span class="n">ctx</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="n">count</span><span class="o">);</span>
      <span class="n">count</span><span class="o">++;</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure><p>Loop가 계속 돌 수 있는지 체크하는 <code class="language-plaintext highlighter-rouge">isRunning</code>을 체크하며 Loop를 돌고 있고, <code class="language-plaintext highlighter-rouge">run</code> 메서드 호출 시 매개변수로 전달받은 <code class="language-plaintext highlighter-rouge">SourceContext</code> 객체인 <code class="language-plaintext highlighter-rouge">ctx</code>의 <code class="language-plaintext highlighter-rouge">collect</code> 메서드를 통해 다음 Operator로 데이터를 전달하는 것을 확인할 수 있다.</p><p>또한 CheckPoint가 수행될 때(<code class="language-plaintext highlighter-rouge">synchronized</code>)는 데이터를 만들어내지 않는 것을 확인할 수 있다.(추후에 확인해봐야겠지만 이 때 Checkpoint Barrier가 전달되는 것으로 예상된다)</p><p><strong><code class="language-plaintext highlighter-rouge">cancel</code> 메서드</strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">void</span> <span class="nf">cancel</span><span class="o">()</span> <span class="o">{</span>
  <span class="n">isRunning</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
<span class="o">}</span></code></pre></figure><p><code class="language-plaintext highlighter-rouge">run</code> 메서드의 Loop에서 사용하는 <code class="language-plaintext highlighter-rouge">isRunning</code> 값을 <code class="language-plaintext highlighter-rouge">false</code>로 변경하므로써, <code class="language-plaintext highlighter-rouge">SourceFunction</code>의 동작을 멈추는 것을 확인할 수 있다.</p><h2 id="architecture-overview">Architecture Overview</h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/FlinkKafkaConsumerClassDiagram.png" alt="Flink Kafka Consumer Class Diagram" /></p><p>Flink의 Kafka Consumer를 구성하는 전반적인 클래스 다이어그램이다. 주요 클래스와 그 역할은 아래와 같다.</p><ul><li><code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>: Flink의 <code class="language-plaintext highlighter-rouge">SourceFunction</code>을 구현하며, 실제 데이터를 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code> 객체를 통해 전달받는다.<li><code class="language-plaintext highlighter-rouge">KafkaFetcher</code>: <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>에서 전달받은 데이터를 <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>에 전달한다.<li><code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>: <code class="language-plaintext highlighter-rouge">KafkaConsumer</code>를 통해 데이터 Pull을 수행하고, 이 데이터를 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 전달한다.<li><code class="language-plaintext highlighter-rouge">Handover</code>: <code class="language-plaintext highlighter-rouge">KafkaConsuerThread</code>와 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>간의 데이터 교환을 위한 Buffer</ul><h2 id="workflow">Workflow</h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/FlinkKafkaConsumerArchitectureOverview.png" alt="Kafka Consumer Architecture Overview" /></p><h3 id="flinkkafkaconsumerbase의-run"><code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>의 <code class="language-plaintext highlighter-rouge">run</code></h3><p><code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>는 <code class="language-plaintext highlighter-rouge">ParallelSourceFunction</code>을 구현한 클래스로써, SourceFunction 관련 메서드들을 구현한다. 위의 사전 지식에서 서술한 것처럼 <code class="language-plaintext highlighter-rouge">run</code> 메서드에서 무한 루프를 돌며 Kafka에서 데이터를 Pull하고, 이 데이터를 <code class="language-plaintext highlighter-rouge">SourceContext</code> 객체의 <code class="language-plaintext highlighter-rouge">collect</code> 메서드를 호출하여 다음 Operator로 전달해야하는 의무를 가진다.</p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="nc">SourceContext</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">sourceContext</span><span class="o">)</span> <span class="kd">throws</span> <span class="nc">Exception</span> <span class="o">{</span>
  <span class="o">...</span> <span class="n">초기화</span> <span class="n">코드</span>
  <span class="k">this</span><span class="o">.</span><span class="na">kafkaFetcher</span> <span class="o">=</span>
                <span class="n">createFetcher</span><span class="o">(</span>
                        <span class="n">sourceContext</span><span class="o">,</span>
                        <span class="n">subscribedPartitionsToStartOffsets</span><span class="o">,</span>
                        <span class="n">watermarkStrategy</span><span class="o">,</span>
                        <span class="o">(</span><span class="nc">StreamingRuntimeContext</span><span class="o">)</span> <span class="n">getRuntimeContext</span><span class="o">(),</span>
                        <span class="n">offsetCommitMode</span><span class="o">,</span>
                        <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getMetricGroup</span><span class="o">().</span><span class="na">addGroup</span><span class="o">(</span><span class="no">KAFKA_CONSUMER_METRICS_GROUP</span><span class="o">),</span>
                        <span class="n">useMetrics</span><span class="o">);</span>
  
  <span class="o">...</span>

  <span class="k">if</span> <span class="o">(</span><span class="n">discoveryIntervalMillis</span> <span class="o">==</span> <span class="no">PARTITION_DISCOVERY_DISABLED</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">kafkaFetcher</span><span class="o">.</span><span class="na">runFetchLoop</span><span class="o">();</span>
  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
    <span class="n">runWithPartitionDiscovery</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure><p>위의 코드를 보면 Kafka로부터 데이터를 Pull하거나, 데이터를 가져오는 Loop를 유지하는 코드는 보이지 않는다. 이는 Flink 관련 기능과 Kafka 관련 기능의 역할 분리를 위한 것으로 판단되며, <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>는 Flink 관련 기능, <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 Kafka 관련 기능으로 역할을 부여받은 것으로 보인다.</p><p>때문에 <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>는 실제 데이터 Fetch를 수행하는 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code> 객체를 초기화하고 실행(<code class="language-plaintext highlighter-rouge">runFetchLoop</code>)하는 역할만을 담당한다.</p><p>후술하겠지만 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 <code class="language-plaintext highlighter-rouge">runFetchLoop</code>는 내부적으로 무한 루프가 수행되는 Blocking 메서드이기 때문에, Pipeline이 종료되기 전까지 <code class="language-plaintext highlighter-rouge">FlinkKafkaConsumerBase</code>의 <code class="language-plaintext highlighter-rouge">run</code> 메서드 또한 종료되지 않는다(Blocking 된다).</p><h3 id="kafkafetcher의-runfetchloop"><code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 <code class="language-plaintext highlighter-rouge">runFetchLoop</code></h3><p><code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 <code class="language-plaintext highlighter-rouge">runFetchLoop</code> 메서드 내부에 Loop 유지, Kafka로부터의 데이터 Pull, 다음 Operator에게 데이터 전달 등의 코드가 있을 것 같지만 그렇지 않다. <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 Kafka로부터 데이터를 Pull하는 역할은 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>에게 위임하고, 자신은 Loop를 유지하며 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>로부터 전달받은 데이터를 다음 Operator에게 전달하는 역할만을 수행한다.</p><p><strong><code class="language-plaintext highlighter-rouge">runFetchLoop</code> 메서드</strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">void</span> <span class="nf">runFetchLoop</span><span class="o">()</span> <span class="kd">throws</span> <span class="nc">Exception</span> <span class="o">{</span>
  <span class="k">try</span> <span class="o">{</span>
    <span class="c1">// kick off the actual Kafka consumer
</span>
    <span class="n">consumerThread</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>

    <span class="k">while</span> <span class="o">(</span><span class="n">running</span><span class="o">)</span> <span class="o">{</span>
      <span class="kd">final</span> <span class="nc">ConsumerRecords</span><span class="o">&lt;</span><span class="kt">byte</span><span class="o">[],</span> <span class="kt">byte</span><span class="o">[]&gt;</span> <span class="n">records</span> <span class="o">=</span> <span class="n">handover</span><span class="o">.</span><span class="na">pollNext</span><span class="o">();</span>

      <span class="k">for</span> <span class="o">(</span><span class="nc">KafkaTopicPartitionState</span><span class="o">&lt;</span><span class="no">T</span><span class="o">,</span> <span class="nc">TopicPartition</span><span class="o">&gt;</span> <span class="n">partition</span> <span class="o">:</span> <span class="n">subscribedPartitionStates</span><span class="o">())</span> <span class="o">{</span>
        <span class="nc">List</span><span class="o">&lt;</span><span class="nc">ConsumerRecord</span><span class="o">&lt;</span><span class="kt">byte</span><span class="o">[],</span> <span class="kt">byte</span><span class="o">[]&gt;&gt;</span> <span class="n">partitionRecords</span> <span class="o">=</span> <span class="n">records</span><span class="o">.</span><span class="na">records</span><span class="o">(</span><span class="n">partition</span><span class="o">.</span><span class="na">getKafkaPartitionHandle</span><span class="o">());</span>
        <span class="n">partitionConsumerRecordsHandler</span><span class="o">(</span><span class="n">partitionRecords</span><span class="o">,</span> <span class="n">partition</span><span class="o">);</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">}</span> <span class="k">finally</span> <span class="o">{</span>
    <span class="n">consumerThread</span><span class="o">.</span><span class="na">shutdown</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="k">try</span> <span class="o">{</span>
    <span class="n">consumerThread</span><span class="o">.</span><span class="na">join</span><span class="o">();</span>
  <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="nc">InterruptedException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
    <span class="nc">Thread</span><span class="o">.</span><span class="na">currentThread</span><span class="o">().</span><span class="na">interrupt</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure><p><code class="language-plaintext highlighter-rouge">runFetchLoop</code> 메서드가 실행된 후 첫번째 행동은 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>를 실행하는 것이다. <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 SourceFunction이 수행되는 Thread와 별도의 Thread로 실행되며, Background에서 Kafka Broker로부터 데이터를 Pull하고 이를 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 전달하는 역할을 수행한다.</p><p><code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 Loop 내에서 <code class="language-plaintext highlighter-rouge">Handover</code> 객체의 <code class="language-plaintext highlighter-rouge">pollNext</code> 메서드를 수행한다. <code class="language-plaintext highlighter-rouge">Handover</code>는 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 데이터를 전달하기 위한 Buffer 역할을 수행한다. <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code> 메서드는 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 Kafka로부터 받아온 데이터가 있을 때 반환되는 Blocking 메서드이다.</p><p><code class="language-plaintext highlighter-rouge">pollNext</code>가 반환한 데이터는 <code class="language-plaintext highlighter-rouge">partitionConsumerRecordsHandler</code> 메서드의 매개변수로 전달되며, <code class="language-plaintext highlighter-rouge">partitionConsumerRecordsHandler</code> 내에서 Deserialize -&gt; Assign Timestamp -&gt; Emit 과정을 거쳐 다음 Operator에게 전달된다.</p><h3 id="kafkaconsumerthread의-역할"><code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>의 역할</h3><p>위의 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 <code class="language-plaintext highlighter-rouge">runFetchLoop</code>의 첫번째 라인에서 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>를 실행했다.</p><p>위에서도 서술했듯 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 SourceFunction과 별도의 Thread로 동작하며, 내부적으로 무한 루프를 돌며 Kafka Broker로부터 데이터를 Pull하고, 받아온 데이터를 <code class="language-plaintext highlighter-rouge">Handover</code> 객체를 통해 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 전달한다.</p><p><strong><code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>의 <code class="language-plaintext highlighter-rouge">run</code> 코드 일부</strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nd">@Override</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">()</span> <span class="o">{</span>
  <span class="o">...</span> <span class="n">생략</span>

  <span class="k">while</span> <span class="o">(</span><span class="n">running</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">records</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">try</span> <span class="o">{</span>
        <span class="n">records</span> <span class="o">=</span> <span class="n">consumer</span><span class="o">.</span><span class="na">poll</span><span class="o">(</span><span class="n">pollTimeout</span><span class="o">);</span>
      <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="nc">WakeupException</span> <span class="n">we</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">continue</span><span class="o">;</span>
      <span class="o">}</span>
    <span class="o">}</span>

    <span class="k">try</span> <span class="o">{</span>
      <span class="n">handover</span><span class="o">.</span><span class="na">produce</span><span class="o">(</span><span class="n">records</span><span class="o">);</span>
      <span class="n">records</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="nc">Handover</span><span class="o">.</span><span class="na">WakeupException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// fall through the loop
</span>
  <span class="o">}</span>
  <span class="o">...</span><span class="na">생략</span>
<span class="o">}</span></code></pre></figure><p>무한 루프가 존재하고, 내부에서 <code class="language-plaintext highlighter-rouge">KafkaConsumer</code> 객체의 <code class="language-plaintext highlighter-rouge">poll</code> 메서드를 호출하여 Kafka Broker로부터 데이터를 Pull 한 뒤, <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">produce</code> 메서드를 호출하여 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 데이터를 전달하는 것을 볼 수 있다.</p><p>재미있는 점은 데이터 Pull 과정에서의 예외 처리 방식이다. <code class="language-plaintext highlighter-rouge">records</code> 객체를 통해 Kafka로부터의 Poll에서의 예외와 Handover의 produce의 예외를 모두 처리하는데, 로직은 다음과 같다.</p><ul><li><code class="language-plaintext highlighter-rouge">records</code>가 null인 경우<ul><li>정상인 경우: 이전 Step에서 Kafka Broker로부터 정상적으로 데이터를 받아오고, 이를 <code class="language-plaintext highlighter-rouge">Handover</code>로 잘 전달하였다.<li>비정상인 경우: 이전 Step에서 Kafka Broker로부터 데이터를 받는데 실패했다.(첫번째 Catch 절에 걸려 continue됨)</ul><li><code class="language-plaintext highlighter-rouge">records</code>가 null이 아닌 경우: 이전 Step에서 <code class="language-plaintext highlighter-rouge">Handover</code>로 데이터를 전달하지 못했다. 때문에 다시 Kafka Broker로부터 데이터를 받아올 필요 없이 전달에 실패한 데이터를 다시 <code class="language-plaintext highlighter-rouge">Handover</code>로 전달한다.</ul><h3 id="handover의-역할과-생산자-소비자-패턴"><code class="language-plaintext highlighter-rouge">Handover</code>의 역할과 생산자-소비자 패턴</h3><p>위에서 서술했듯 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 Loop와 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>의 Loop는 서로 다른 Thread에서 실행된다. Flink에서는 두 Thread간의 데이터 교환을 생산자-소비자 패턴을 통해 구현하였다.</p><p>여기서는 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 데이터를 생성하는 생산자(Producer) 역할을 하고, <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 소비자(Consumer) 역할을 한다. 생성자-소비자 패턴에서 데이터 교환에는 보통 <code class="language-plaintext highlighter-rouge">BlockingQueue</code>가 사용되는데, 여기서는 <code class="language-plaintext highlighter-rouge">Handover</code> 객체가 <code class="language-plaintext highlighter-rouge">BlockingQueue</code>의 역할을 대체한다.</p><p><code class="language-plaintext highlighter-rouge">Handover</code>는 <code class="language-plaintext highlighter-rouge">BlockingQueue</code>의 역할을 수행하기 위해 내부적으로 2개의 변수를 활용한다.</p><ul><li><code class="language-plaintext highlighter-rouge">lock</code>: <code class="language-plaintext highlighter-rouge">Object</code> 타입의 객체이며, <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>와 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 서로의 동작이 끝나길 기다리거나(<code class="language-plaintext highlighter-rouge">wait</code>), 상대방에게 동작이 완료되었다고 알려주는(<code class="language-plaintext highlighter-rouge">notifyAll</code>) 역할을 수행한다.<li><code class="language-plaintext highlighter-rouge">next</code>: <code class="language-plaintext highlighter-rouge">ConsumerRecords</code> 타입의 객체이며, <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 Kafka Broker로부터 Fetch 해온 데이터를 넣거나 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 다음 Operator에게 데이터를 전달할 때 조회하여 가져간다.</ul><p>설명만으로는 이해가 어려우니 상황 별로 나누어 그림으로 설명 후 코드를 살펴보도록 한다.</p><blockquote><p><code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 Kafka Broker로부터 Pull한 데이터를 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">produce</code> 호출을 통해 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>에게 전달하려 하고, <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code> 호출을 통해 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 넣어놓은 데이터가 있다면 가져가려는 상황</p></blockquote><p>1.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 Kafka Broker로부터 데이터를 가져와 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">produce</code> 호출. <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 <code class="language-plaintext highlighter-rouge">Handover</code>의 lock을 획득하여 Critical Section에 진입.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover1.png" alt="Handover1" /></p><p>2.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 1과 동시에 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code> 호출. <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 <code class="language-plaintext highlighter-rouge">Handover</code>의 lock을 획득하려 하나 이미 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 lock을 선점하고 있기 때문에 Critical Section에 들어가지 못하고 Blocking</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover2.png" alt="Handover2" /></p><p>3.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>에서 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">records</code>에 Kafka Broker로부터 Pull했던 데이터를 할당. 작업이 완료되었으므로 lock을 반환하고 다음 데이터를 Fetch하러 감</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover3.png" alt="Handover3" /></p><p>4.3번째 Step에서 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 lock을 반환했기 때문에, <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 lock 획득에 성공하여 Critical Section에 진입.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover4.png" alt="Handover4" /></p><p>5.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 <code class="language-plaintext highlighter-rouge">records</code>에 데이터가 있는 것(<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 넣어준)을 확인하고 이를 가져감. 또한 lock은 반환함</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover5.png" alt="Handover5" /></p><p>6.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>의 데이터 Pull이 오래 걸리는 상황에서 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 이전 Step의 모든 데이터 처리. 이에 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 다시 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code> 호출</p><p>7.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 <code class="language-plaintext highlighter-rouge">Handover</code>의 lock을 획득하고 <code class="language-plaintext highlighter-rouge">records</code>를 확인하지만 데이터가 존재하지 않음(null). 이에 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">wait</code>을 호출하여 대기 상태로 진입</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover7.png" alt="Handover7" /></p><p>8.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 Kafka Broker로부터 데이터 Pull 완료. <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">produce</code> 호출</p><p>9.7번째 Step에서 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">wait</code>을 호출하며 <code class="language-plaintext highlighter-rouge">lock</code> 소유권을 반환했기 때문에 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 lock을 획득하며 Critical Section 진입.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover9.png" alt="Handover9" /></p><p>10.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 자신이 Kafka Broker로부터 Pull한 데이터를 <code class="language-plaintext highlighter-rouge">records</code>에 할당. 이후 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">notifyAll</code>을 호출하며 종료</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/flink/handover10.png" alt="Handover10" /></p><p>11.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 10번째 Step에서 호출된 <code class="language-plaintext highlighter-rouge">notifyAll</code>에 의해 WAIT에서 RUNNING 상태로 진입. 5번째 Step 반복</p><p>12.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 1, 3번째 Step 수행</p><p>13.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 수행이 늦어져 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 다시 한번 1번째 Step 수행. 그러나 <code class="language-plaintext highlighter-rouge">records</code>에 아직 가져가지 않은 데이터가 존재하는 것을 확인하여 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">wait</code>을 호출하여 대기 상태로 진입</p><p>14.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>의 데이터 처리가 완료되어 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code> 호출</p><p>15.13번째 Step에서 <code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>가 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">wait</code>을 호출하며 <code class="language-plaintext highlighter-rouge">lock</code>의 소유권을 반환했기 때문에 <code class="language-plaintext highlighter-rouge">KafkaFetcher</code>가 lock을 획득하며 Critical Section 진입</p><p>16.<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>는 5번 Step을 반복. 이 때 <code class="language-plaintext highlighter-rouge">lock</code>의 <code class="language-plaintext highlighter-rouge">notifyAll</code>을 호출</p><p>17.<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>는 16번째 Step에서 호출된 <code class="language-plaintext highlighter-rouge">notifyAll</code>에 의해 WAIT에서 RUNNING 상태로 진입. 1, 3번째 Step 진행</p><p>위 과정을 코드로 살펴보면 아래와 같다.</p><p><strong>생산자(<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>)에서 사용하는 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">produce</code></strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">void</span> <span class="nf">produce</span><span class="o">(</span><span class="kd">final</span> <span class="nc">ConsumerRecords</span><span class="o">&lt;</span><span class="kt">byte</span><span class="o">[],</span> <span class="kt">byte</span><span class="o">[]&gt;</span> <span class="n">element</span><span class="o">)</span>
  <span class="kd">throws</span> <span class="nc">InterruptedException</span><span class="o">,</span> <span class="nc">WakeupException</span><span class="o">,</span> <span class="nc">ClosedException</span> <span class="o">{</span>

  <span class="n">checkNotNull</span><span class="o">(</span><span class="n">element</span><span class="o">);</span>

  <span class="kd">synchronized</span> <span class="o">(</span><span class="n">lock</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">next</span> <span class="o">!=</span> <span class="kc">null</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">wakeupProducer</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">lock</span><span class="o">.</span><span class="na">wait</span><span class="o">();</span>
    <span class="o">}</span>

    <span class="n">wakeupProducer</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>

    <span class="c1">// if there is still an element, we must have been woken up
</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">next</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">throw</span> <span class="k">new</span> <span class="nf">WakeupException</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="c1">// if there is no error, then this is open and can accept this element
</span>
    <span class="k">else</span> <span class="nf">if</span> <span class="o">(</span><span class="n">error</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">next</span> <span class="o">=</span> <span class="n">element</span><span class="o">;</span>
      <span class="n">lock</span><span class="o">.</span><span class="na">notifyAll</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="c1">// an error marks this as closed for the producer
</span>
    <span class="k">else</span> <span class="o">{</span>
      <span class="k">throw</span> <span class="k">new</span> <span class="nf">ClosedException</span><span class="o">();</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure><ul><li><code class="language-plaintext highlighter-rouge">synchronized (lock)</code> 구문을 통해 소비자(<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>)와의 Race Condition을 없애고 있다.<li><code class="language-plaintext highlighter-rouge">while(next != null &amp;&amp; !wakeupProducer)</code> 구문을 통해 소비자가 데이터를 가져가지 않은 경우 lock을 반환하고 데이터를 가져갈 때까지 대기(<code class="language-plaintext highlighter-rouge">lock.wait</code>)하는 것을 볼 수 있다.<li>데이터를 넣은 후(<code class="language-plaintext highlighter-rouge">next = element</code>)에는 혹시나 데이터를 넣기를 기다리고 있을 소비자를 깨우기 위해 <code class="language-plaintext highlighter-rouge">lock.notifyAll</code>을 호출하는 것을 볼 수 있다.</ul><p><strong>소비자(<code class="language-plaintext highlighter-rouge">KafkaFetcher</code>)에서 사용하는 <code class="language-plaintext highlighter-rouge">Handover</code>의 <code class="language-plaintext highlighter-rouge">pollNext</code></strong></p><figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="nc">ConsumerRecords</span><span class="o">&lt;</span><span class="kt">byte</span><span class="o">[],</span> <span class="kt">byte</span><span class="o">[]&gt;</span> <span class="nf">pollNext</span><span class="o">()</span> <span class="kd">throws</span> <span class="nc">Exception</span> <span class="o">{</span>
  <span class="kd">synchronized</span> <span class="o">(</span><span class="n">lock</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">next</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">&amp;&amp;</span> <span class="n">error</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">lock</span><span class="o">.</span><span class="na">wait</span><span class="o">();</span>
    <span class="o">}</span>

    <span class="nc">ConsumerRecords</span><span class="o">&lt;</span><span class="kt">byte</span><span class="o">[],</span> <span class="kt">byte</span><span class="o">[]&gt;</span> <span class="n">n</span> <span class="o">=</span> <span class="n">next</span><span class="o">;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">n</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">next</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
      <span class="n">lock</span><span class="o">.</span><span class="na">notifyAll</span><span class="o">();</span>
      <span class="k">return</span> <span class="n">n</span><span class="o">;</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="nc">ExceptionUtils</span><span class="o">.</span><span class="na">rethrowException</span><span class="o">(</span><span class="n">error</span><span class="o">,</span> <span class="n">error</span><span class="o">.</span><span class="na">getMessage</span><span class="o">());</span>
      <span class="k">return</span> <span class="nc">ConsumerRecords</span><span class="o">.</span><span class="na">empty</span><span class="o">();</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure><ul><li><code class="language-plaintext highlighter-rouge">synchronized (lock)</code> 구문을 통해 생산자(<code class="language-plaintext highlighter-rouge">KafkaConsumerThread</code>)와의 Race Condition을 없애고 있다.<li><code class="language-plaintext highlighter-rouge">while(next == null &amp;&amp; error == null)</code> 구문을 통해 생산자가 데이터를 넣어주지 않은 경우 lock을 반환하고 데이터를 넣어줄 때까지 대기(<code class="language-plaintext highlighter-rouge">lock.wait</code>)하는 것을 볼 수 있다.<li>데이터를 처리한 뒤(<code class="language-plaintext highlighter-rouge">ConsumerRecords&lt;byte[], byte[]&gt; n = next</code>)에는 혹시나 데이터를 소비하기를 기다리고 있을 생산자를 깨우기 위해 <code class="language-plaintext highlighter-rouge">lock.notifyAll</code>을 호출하는 것을 볼 수 있다.</ul><h1 id="정리">정리</h1><p>웹이나 ETL 개발 등에는 잘 사용하지 않는 생산자-소비자 패턴과 <code class="language-plaintext highlighter-rouge">Object</code>의 <code class="language-plaintext highlighter-rouge">wait</code>이나 <code class="language-plaintext highlighter-rouge">notifyAll</code>의 자세한 사용법을 알아서 좋았다.</p><p>또한 Kafka &lt;-&gt; Flink 혹은 Flink 내의 Source &lt;-&gt; Operator 간의 속도가 다를 경우 어떤 관점에서는 마이크로 배치처럼 보일 수도 있겠구나 라는 생각도 들었다.</p><p>다음 글에는 Flink Kafka Consumer가 Checkpoint를 어떻게 처리하는지 알아볼 예정이다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/flink/'>flink</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/flink/" class="post-tag no-text-decoration" >flink</a> <a href="/tags/kafka/" class="post-tag no-text-decoration" >kafka</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Flink Concept - Flink의 Kafka Consumer 동작 방식(1) - leeyh0216's devlog&url=https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Flink Concept - Flink의 Kafka Consumer 동작 방식(1) - leeyh0216's devlog&u=https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Flink Concept - Flink의 Kafka Consumer 동작 방식(1) - leeyh0216's devlog&url=https://leeyh0216.github.io/posts/flink_kafka_consumer_works_1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ps/">ps</a> <a class="post-tag" href="/tags/spring/">spring</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/apache-spark/">apache-spark</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/kafka/">kafka</a> <a class="post-tag" href="/tags/apache-druid/">apache-druid</a> <a class="post-tag" href="/tags/string/">string</a> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/docker/">docker</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/spark-structured-streaming-microbatch/"><div class="card-body"> <span class="timeago small" > Sep 2, 2022 <i class="unloaded">2022-09-02T23:55:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spark Structured Streaming의 MicroBatch 동작 원리 알아보기</h3><div class="text-muted small"><p> 이 글의 내용은 MicroBatchExecution - Stream Execution Engine of Micro-Batch Stream Processing을 참고하여 작성하였습니다. Spark Structured Streaming의 MicroBatch 동작 원리 알아보기 MicroBatch 기반의 Spark Structured Stream...</p></div></div></a></div><div class="card"> <a href="/posts/kafka_concept/"><div class="card-body"> <span class="timeago small" > May 2, 2020 <i class="unloaded">2020-05-02T15:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Kafka의 Topic, Partition, Segment, Message</h3><div class="text-muted small"><p> Topic Topic은 메시지가 발행되는 카테고리 혹은 Feed로써, 메시지들의 스트림으로 볼 수 있다. 다양한 생산자(Producer)와 소비자(Consumer)들이 Topic에 메시지를 발행하거나, Topic으로부터 발행된 메시지를 가져와 사용한다. Kafka의 문서나 관련 서적에서 Kafka를 분산 커밋 로그(Distributed c...</p></div></div></a></div><div class="card"> <a href="/posts/kafka_producer/"><div class="card-body"> <span class="timeago small" > May 3, 2020 <i class="unloaded">2020-05-03T18:10:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>간단한 Kafka Producer를 만들고 동작원리를 알아보자</h3><div class="text-muted small"><p> Kafka Producer 메시지 발행/구독 시스템에서 Producer는 메시지의 발행을 수행하는 컴포넌트이다. 이 글에서는 Kafka Client API를 사용하여 Kafka Producer를 만들어보고, 메시지가 실제로 어떤 과정을 거쳐 전달되는지 알아본다. build.gradle에 Kafka Client 의존성 추가하기 build.grad...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/singleton/" class="btn btn-outline-primary" prompt="Older"><p>싱글톤 패턴</p></a> <a href="/posts/spark-structured-streaming-kafka-sink/" class="btn btn-outline-primary" prompt="Newer"><p>Apache Spark - Spark Structured Streaming Kafka Sink는 Exactly-Once를 지원하지 않는다</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://twitter.com/username">leeyh0216</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/ps/">ps</a> <a class="post-tag" href="/tags/spring/">spring</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/apache-spark/">apache spark</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/kafka/">kafka</a> <a class="post-tag" href="/tags/apache-druid/">apache druid</a> <a class="post-tag" href="/tags/string/">string</a> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/docker/">docker</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leeyh0216.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
