<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8829030678254956" crossorigin="anonymous"></script><meta name="pv-proxy-endpoint" content=""><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Spark UDF와 DataSet에서의 NULL 처리" /><meta name="author" content="leeyh0216" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다." /><meta property="og:description" content="개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다." /><link rel="canonical" href="https://leeyh0216.github.io/posts/spark-udf-null/" /><meta property="og:url" content="https://leeyh0216.github.io/posts/spark-udf-null/" /><meta property="og:site_name" content="leeyh0216’s devlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2018-11-13T10:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Spark UDF와 DataSet에서의 NULL 처리" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@leeyh0216" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"leeyh0216"},"description":"개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다.","url":"https://leeyh0216.github.io/posts/spark-udf-null/","@type":"BlogPosting","headline":"Spark UDF와 DataSet에서의 NULL 처리","dateModified":"2018-11-13T10:00:00+09:00","datePublished":"2018-11-13T10:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://leeyh0216.github.io/posts/spark-udf-null/"},"@context":"https://schema.org"}</script><title>Spark UDF와 DataSet에서의 NULL 처리 | leeyh0216's devlog</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script async src="/assets/js/dist/pvreport.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-129061352-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-129061352-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">leeyh0216's devlog</a></div><div class="site-subtitle font-italic">개발/일상 블로그</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/leeyh0216" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EC%9A%A9%ED%99%98-%EC%9D%B4-84222a119/" aria-label="linkedin" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['leeyh0216','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Spark UDF와 DataSet에서의 NULL 처리</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Spark UDF와 DataSet에서의 NULL 처리</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> leeyh0216 </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Nov 13, 2018, 10:00 AM +0900" prep="on" > Nov 13, 2018 <i class="unloaded">2018-11-13T10:00:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1497 words">8 min</span> <span id="pv" class="pageviews"><i class="fas fa-spinner fa-spin fa-fw"></i></span></div></div><div class="post-content"><h1 id="개요">개요</h1><p>Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다.</p><p>SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다.</p><p>다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다.</p><h1 id="udf에서의-null-처리">UDF에서의 NULL 처리</h1><h2 id="string-type">String Type</h2><p>String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다.</p><p>주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT "hello" AS a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">he</span>    <span class="o">|</span>
<span class="o">+------+</span></code></pre></figure><p>정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nc">Seq</span><span class="o">(</span><span class="s">"hello"</span><span class="o">,</span> <span class="kc">null</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">SparkException</span><span class="k">:</span> <span class="kt">Failed</span> <span class="kt">to</span> <span class="kt">execute</span> <span class="kt">user</span> <span class="kt">defined</span> <span class="kt">function</span><span class="o">(</span><span class="kt">$anonfun$1:</span> <span class="o">(</span><span class="kt">string</span><span class="o">)</span> <span class="o">=&gt;</span> <span class="kt">string</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">.</span><span class="py">catalyst</span><span class="o">.</span><span class="py">expressions</span><span class="o">.</span><span class="py">ScalaUDF</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nv">ScalaUDF</span><span class="o">.</span><span class="py">scala</span><span class="k">:</span><span class="err">1058</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">.</span><span class="py">catalyst</span><span class="o">.</span><span class="py">expressions</span><span class="o">.</span><span class="py">Alias</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nv">namedExpressions</span><span class="o">.</span><span class="py">scala</span><span class="k">:</span><span class="err">139</span><span class="o">)</span>
  <span class="o">...</span><span class="py">생략</span>
<span class="nc">Caused</span> <span class="n">by</span><span class="k">:</span> <span class="kt">java.lang.NullPointerException</span>
  <span class="n">at</span> <span class="nv">$anonfun$1</span><span class="o">.</span><span class="py">apply</span><span class="o">(&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">23</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">$anonfun$1</span><span class="o">.</span><span class="py">apply</span><span class="o">(&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">23</span><span class="o">)</span>
  <span class="o">...</span><span class="py">생략</span></code></pre></figure><p>위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다.</p><p>이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nf">if</span><span class="o">(</span><span class="n">s</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="s">""</span> <span class="k">else</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nc">Seq</span><span class="o">(</span><span class="s">"hello"</span><span class="o">,</span> <span class="kc">null</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
    실행 결과
    +------+
    |UDF(a)|
    +------+
    |he    |
    |      |
    +------+
*****************/</span></code></pre></figure><p>위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다.</p><h2 id="int-long-등의-숫자-primitive-type">Int, Long 등의 숫자 Primitive Type</h2><p>오늘 삽질의 원인이 되었던 Integer, Long 타입이다.</p><p>처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다.</p><p>실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="o">+</span><span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select 1 as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|2     |
+------+
*****************/</span></code></pre></figure><p>정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다.</p><p>그렇다면 a 필드에 null 값을 넣어보면 어떨까?</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="o">+</span><span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|null  |
+------+
*****************/</span></code></pre></figure><p>null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다.</p><p>이유는 아래와 같은 코드를 작성해보면 알 수 있다.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">tmp</span> <span class="k">=</span> <span class="nv">null</span><span class="o">.</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">tmp</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">1</span></code></pre></figure><p>Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다.</p><p>당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다.</p><p>이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다.</p><p>위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span>
<span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">java.lang.Integer</span> <span class="o">=&gt;</span> <span class="nf">if</span><span class="o">(</span><span class="n">a</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="mi">1</span> <span class="k">else</span> <span class="nv">a</span><span class="o">.</span><span class="py">intValue</span><span class="o">()</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|1     |
+------+
*****************/</span></code></pre></figure><p>의도한 대로 동작하는 것을 확인할 수 있다.</p><h1 id="dataset-사용-시의-null-처리">DataSet 사용 시의 null 처리</h1><p>만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까?</p><p>아래와 같은 코드를 이용하여 테스트 해 보았다.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="nv">r</span><span class="o">.</span><span class="py">b</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="nv">r</span><span class="o">.</span><span class="py">c</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">))</span>

<span class="nc">Driver</span> <span class="n">stacktrace</span><span class="k">:</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1602</span><span class="o">)</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1590</span><span class="o">)</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1589</span><span class="o">)</span>
  <span class="kt">...</span> <span class="kt">생략</span>
<span class="nc">Caused</span> <span class="n">by</span><span class="k">:</span> <span class="kt">java.lang.NullPointerException:</span> <span class="kt">Null</span> <span class="kt">value</span> <span class="kt">appeared</span> <span class="kt">in</span> <span class="kt">non-nullable</span> <span class="kt">field:</span>
<span class="kt">-</span> <span class="kt">field</span> <span class="o">(</span><span class="kt">class:</span> <span class="err">"</span><span class="kt">scala.Int</span><span class="err">"</span><span class="o">,</span> <span class="kt">name:</span> <span class="err">"</span><span class="kt">a</span><span class="err">"</span><span class="o">)</span>
<span class="o">-</span> <span class="n">root</span> <span class="n">class</span><span class="k">:</span> <span class="err">"</span><span class="kt">A</span><span class="err">"</span>
<span class="kt">If</span> <span class="kt">the</span> <span class="kt">schema</span> <span class="kt">is</span> <span class="kt">inferred</span> <span class="kt">from</span> <span class="kt">a</span> <span class="kt">Scala</span> <span class="kt">tuple/case</span> <span class="kt">class</span><span class="o">,</span> <span class="n">or</span> <span class="n">a</span> <span class="nc">Java</span> <span class="n">bean</span><span class="o">,</span> <span class="n">please</span> <span class="k">try</span> <span class="n">to</span> <span class="n">use</span> <span class="nv">scala</span><span class="o">.</span><span class="py">Option</span><span class="o">[</span><span class="k">_</span><span class="o">]</span> <span class="n">or</span> <span class="n">other</span> <span class="n">nullable</span> <span class="nf">types</span> <span class="o">(</span><span class="nv">e</span><span class="o">.</span><span class="py">g</span><span class="o">.</span> <span class="nv">java</span><span class="o">.</span><span class="py">lang</span><span class="o">.</span><span class="py">Integer</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">int</span><span class="o">/</span><span class="nv">scala</span><span class="o">.</span><span class="py">Int</span><span class="o">).</span>
<span class="o">...</span> <span class="n">생략</span></code></pre></figure><p>위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다.</p><p>그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자.</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"""</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="nc">Some</span><span class="o">((</span><span class="nv">r</span><span class="o">.</span><span class="py">b</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0L</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)),</span> <span class="nv">r</span><span class="o">.</span><span class="py">c</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">)).</span><span class="py">toDF</span><span class="o">().</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="o">+---+---+--------------+</span>
<span class="o">|</span><span class="n">a</span>  <span class="o">|</span><span class="n">b</span>  <span class="o">|</span><span class="n">c</span>             <span class="o">|</span>
<span class="o">+---+---+--------------+</span>
<span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="kc">null</span> <span class="n">is</span> <span class="n">string</span><span class="o">|</span>
<span class="o">+---+---+--------------+</span></code></pre></figure><p>위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다.</p><p>String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다.</p><p>String도 Option으로 처리해보면 어떨까?</p><figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"""</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="nc">Some</span><span class="o">((</span><span class="nv">r</span><span class="o">.</span><span class="py">b</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0L</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)),</span> <span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">c</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="s">""</span><span class="o">)</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">))).</span><span class="py">toDF</span><span class="o">().</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+---+---+----------+</span>
<span class="o">|</span><span class="n">a</span>  <span class="o">|</span><span class="n">b</span>  <span class="o">|</span><span class="n">c</span>         <span class="o">|</span>
<span class="o">+---+---+----------+</span>
<span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="mi">1</span>  <span class="o">|</span> <span class="n">is</span> <span class="n">string</span><span class="o">|</span>
<span class="o">+---+---+----------+</span></code></pre></figure><p>위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다.</p><h1 id="결론">결론</h1><p>따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는</p><ul><li>UDF 작성 시 Primitive Type이 아닌 Object Type 사용<li>Case Class 사용 시 Option 사용</ul><p>을 유의해주어야 한다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/apache-spark/" class="post-tag no-text-decoration" >apache-spark</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Spark UDF와 DataSet에서의 NULL 처리 - leeyh0216's devlog&url=https://leeyh0216.github.io/posts/spark-udf-null/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Spark UDF와 DataSet에서의 NULL 처리 - leeyh0216's devlog&u=https://leeyh0216.github.io/posts/spark-udf-null/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Spark UDF와 DataSet에서의 NULL 처리 - leeyh0216's devlog&url=https://leeyh0216.github.io/posts/spark-udf-null/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ps/">ps</a> <a class="post-tag" href="/tags/spring/">spring</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/apache-spark/">apache-spark</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/kafka/">kafka</a> <a class="post-tag" href="/tags/apache-druid/">apache-druid</a> <a class="post-tag" href="/tags/string/">string</a> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/docker/">docker</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/spark-structured-streaming-microbatch/"><div class="card-body"> <span class="timeago small" > Sep 2, 2022 <i class="unloaded">2022-09-02T23:55:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spark Structured Streaming의 MicroBatch 동작 원리 알아보기</h3><div class="text-muted small"><p> 이 글의 내용은 MicroBatchExecution - Stream Execution Engine of Micro-Batch Stream Processing을 참고하여 작성하였습니다. Spark Structured Streaming의 MicroBatch 동작 원리 알아보기 MicroBatch 기반의 Spark Structured Stream...</p></div></div></a></div><div class="card"> <a href="/posts/spark-private-s3-migration/"><div class="card-body"> <span class="timeago small" > Sep 3, 2022 <i class="unloaded">2022-09-03T15:05:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Apache Spark과 S3 Compatible Object Storage 연동 시 Custom Endpoint 이슈</h3><div class="text-muted small"><p> Apache Spark과 S3 Compatible Object Storage 연동 시 Custom Endpoint 이슈 사내에서 개발하는 시스템에서 Apache Spark과 S3 Compatible Object Storage인 Ceph를 연동해야 할 일이 생겼다. Ceph는 S3 Compatible한 Gateway를 제공하기 때문에 Apache S...</p></div></div></a></div><div class="card"> <a href="/posts/spark-adaptive-query-execution/"><div class="card-body"> <span class="timeago small" > Sep 4, 2022 <i class="unloaded">2022-09-04T00:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Apache Spark 3.0에서 도입된 Adaptive Query Execution 알아보기</h3><div class="text-muted small"><p> 이 글은 Adaptive Query Execution: Speeding up Spark SQL at Runtime을 참고하여 작성하였습니다. Adaptive Query Execution 아마 마이크 타이슨에 대해 아는 사람이라면 아래의 명언을 들어본 적이 있을 것이다. Everyone has a plan, until they get pun...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/spring-cloud-zuul/" class="btn btn-outline-primary" prompt="Older"><p>Spring Cloud - Zuul(1)</p></a> <a href="/posts/elasticsearch-1/" class="btn btn-outline-primary" prompt="Newer"><p>ElasticSearch + MetricBeat + Kibana로 서버 모니터링하기</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://twitter.com/username">leeyh0216</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/ps/">ps</a> <a class="post-tag" href="/tags/spring/">spring</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/apache-spark/">apache spark</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/kafka/">kafka</a> <a class="post-tag" href="/tags/apache-druid/">apache druid</a> <a class="post-tag" href="/tags/string/">string</a> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/docker/">docker</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leeyh0216.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
