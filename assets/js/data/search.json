[ { "title": "JVM Heap과 GC를 다른 관점에서 바라보기", "url": "/posts/heap-and-gc/", "categories": "", "tags": "java, gc, heap", "date": "2024-07-14 17:00:00 +0900", "snippet": "개요Garbage Collection을 검색해보면 대부분의 글이 특정 GC(CMS, G1, Z 등)의 배경이 되는 이론(ex. Generational Collection Theory)이나, 알고리즘(ex. Mark-Sweep), 튜닝 등에 대한 내용을 다루고 있다. 그리고 해당 이론에 근거하여 Heap 메모리의 구조를 설명하다보니, Heap 영역을 Eden, Old, Perm 등으로 나누어 생각하는 것이 일반화된 것 같다.최근에 JVM 밑바닥까지 파헤치기 라는 책을 읽다가 다음과 같은 문장을 읽었다. 이 영역 구분(Eden,..." }, { "title": "protobuf 2.5 빌드하기(Apple Silicon)", "url": "/posts/protobuf_2_5_build/", "categories": "", "tags": "hadoop", "date": "2024-03-10 12:10:00 +0900", "snippet": " 이 문제는 Apple Silicon(M1, M2, M3 등)에서만 발생합니다. Ubuntu나 Intel Mac에서는 발생하지 않을 수 있음을 유의하시기 바랍니다.Hadoop 3.2.2 버전을 빌드하려다보니, 아래와 같은 메시지가 발생하며 빌드에 실패하였다.[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.2.2:protoc (compile-protoc) on project hadoop-yarn-api: org.apache.maven.plugin...." }, { "title": "데이터 엔지니어를 위한 CMU Intro to Database Systems#2", "url": "/posts/cmu_intro_to_database_2/", "categories": "", "tags": "database", "date": "2023-12-22 11:00:00 +0900", "snippet": "Lecture #10: Sorting &amp;amp; Aggregation Algorithms참고자료: Youtube - F2023 #10 - Sorting &amp;amp; Aggregation Algorithms (CMU Intro to Database Systems)Query PlanSQL문을 실행하면, DBMS는 쿼리를 Compile하여 Query Plan으로 변환한다. Query Plan은 Operator(Relational Operator)로 구성된 Tree를 의미한다.SELECT R.id, S.cdateFROM ..." }, { "title": "데이터 엔지니어를 위한 CMU Intro to Database Systems#1", "url": "/posts/cmu_intro_to_database_1/", "categories": "", "tags": "database", "date": "2023-12-21 18:00:00 +0900", "snippet": "들어가며2014년 2학기에 데이터베이스 개론을 수강했던 기억이 있다. 당시에는 개념적인 것보다 실제 동작하는 프로그램에 RDBMS를 연동하여 결과를 얻어내는 것에 더 관심이 있었다. 그러나 몇년 간 데이터 엔지니어 업무를 수행하다보니, 최적화를 위해서는 엔진 내부에 대한 이해가 중요하다는 결론이 내려졌다.내가 다루는 엔진이 대부분 분산 데이터베이스 엔진이기는 하지만, 기본적인 개념은 일반 데이터베이스를 구성하는 이론과 크게 차이가 없는 것을 확인하였다. 어떤 자료를 통해 공부할지 고민하다가 카네기 멜론 대학교의 “CMU Int..." }, { "title": "Trino Summit 2023 발표 회고", "url": "/posts/trino-summit-2023/", "categories": "", "tags": "java, trino", "date": "2023-12-16 18:00:00 +0900", "snippet": "Trino Summit 20232023년 12월 13 ~ 14일(한국 기준 14 ~ 15일)에 Trino Summit 2023이 온라인에서 열렸다.Trino 블로그에도 Trino Summit 2023 nears with an awesome lineup에 “SK Telecom: Unstructured data analysis using polymorphic table functions in Trino” 이라는 제목으로 소개되어 있다.2023년 12월 16일 18시 기준 Youtube Trino 채널에 Trino Summit 20..." }, { "title": "ORC Spec을 보고 Reader/Writer를 구현해보기 - PostScript", "url": "/posts/Orc-Impl-1/", "categories": "", "tags": "java, orc, hadoop", "date": "2023-01-01 01:00:00 +0900", "snippet": "개요대부분의 오픈소스 ETL 프로젝트(Spark, Flink 등)들에서는 ORC, Parquet 등의 컬럼 기반 파일 포맷의 읽기/쓰기를 지원하며, 고수준 API를 통해 간단히 특정 포맷으로의 읽기/쓰기를 수행할 수 있다.-- ORC의 읽기spark.read.orc(&quot;PATH_TO_READ&quot;).show(100, false)-- ORC의 쓰기df.write.orc(&quot;PATH_TO_WRITE&quot;)현업에서 코드를 작성하는 이유는 “비즈니스 목표를 달성하기 위함” 이기 때문에, 각 오픈소스 프로젝트에..." }, { "title": "Java Object의 Memory Layout과 Trino의 Slice", "url": "/posts/trino-slice/", "categories": "", "tags": "java, trino, airlift, unsafe", "date": "2022-12-24 15:00:00 +0900", "snippet": "Java Object의 Memory LayoutData structure alignmentWORDCPU에서 어떤 작업을 하기 위해서는 메모리 영역의 데이터를 레지스터로 옮겨야 한다. 이 때 CPU가 레지스터로 데이터를 옮겨오는 단위를 WORD라고 한다. WORD의 크기는 CPU마다 다르다. 32bit CPU에서는 WORD의 크기가 32bit 64bit CPU에서는 WORD의 크기가 64bit다만 Intel x86에서 WORD=16bit로 정해놓은 MACRO가 존재하였기 때문에, 어셈블리 상에서는 아직도 WORD=16bit ..." }, { "title": "Apache Spark 3.0에서 도입된 Adaptive Query Execution 알아보기", "url": "/posts/spark-adaptive-query-execution/", "categories": "", "tags": "apache-spark", "date": "2022-09-04 00:20:00 +0900", "snippet": " 이 글은 Adaptive Query Execution: Speeding up Spark SQL at Runtime을 참고하여 작성하였습니다.Adaptive Query Execution아마 마이크 타이슨에 대해 아는 사람이라면 아래의 명언을 들어본 적이 있을 것이다.Everyone has a plan, until they get punched in the mouth - 누구나 그럴싸한 계획을 가지고 있다. 주둥이를 처맞기 전까진, 마이크 타이슨실제로는 아래와 같은 말이지만…Everyone has a plan until the..." }, { "title": "Apache Spark과 S3 Compatible Object Storage 연동 시 Custom Endpoint 이슈", "url": "/posts/spark-private-s3-migration/", "categories": "", "tags": "apache-spark, s3, hadoop", "date": "2022-09-03 15:05:00 +0900", "snippet": "Apache Spark과 S3 Compatible Object Storage 연동 시 Custom Endpoint 이슈사내에서 개발하는 시스템에서 Apache Spark과 S3 Compatible Object Storage인 Ceph를 연동해야 할 일이 생겼다.Ceph는 S3 Compatible한 Gateway를 제공하기 때문에 Apache Spark 실행 시 spark.hadoop.fs.s3a.endpoint에 해당 Gateway의 주소를 설정해주고, Access Key와 Secret Key까지 설정해주면 금방 될 거라 생각..." }, { "title": "Spark Structured Streaming의 MicroBatch 동작 원리 알아보기", "url": "/posts/spark-structured-streaming-microbatch/", "categories": "", "tags": "apache-spark, kafka", "date": "2022-09-02 23:55:00 +0900", "snippet": " 이 글의 내용은 MicroBatchExecution - Stream Execution Engine of Micro-Batch Stream Processing을 참고하여 작성하였습니다.Spark Structured Streaming의 MicroBatch 동작 원리 알아보기MicroBatch 기반의 Spark Structured Streaming이 어떻게 동작하는지 알아본다.MicroBatch Stream ProcessingWikipedia - Streaming Data에서 정의한 스트리밍 데이터는 “서로 다른 소스에서 발생하..." }, { "title": "Apache Spark - Spark Structured Streaming Kafka Sink는 Exactly-Once를 지원하지 않는다", "url": "/posts/spark-structured-streaming-kafka-sink/", "categories": "", "tags": "apache-spark, kafka", "date": "2022-07-31 13:00:00 +0900", "snippet": " Spark Structured Streaming Kafka Sink의 동작에 대해 알아봅니다. 기본적인 사용법보다는 내부적으로 Kafka Sink가 어떻게 동작하는지, 왜 Spark Structured Streaming Kafka Sink의 한계에 대한 이야기를 다루고 있습니다.Spark Structured Streaming Kafka Sink회사에서 최근 Spark Structured Streaming을 사용하고 있고, 내가 개발하는 모듈은 Source와 Sink 모두 Kafka를 사용하고 있다. Lambda Archit..." }, { "title": "Flink Concept - Flink의 Kafka Consumer 동작 방식(1)", "url": "/posts/flink_kafka_consumer_works_1/", "categories": "flink", "tags": "flink, kafka", "date": "2021-04-15 00:50:00 +0900", "snippet": "개요Flink의 FlinkKafkaConsumer와 같은 High Level API를 사용하다보면 궁금해지는 점들이 있다. Kafka의 Consumer는 Pull 방식으로 데이터를 가져오는데, Flink에서는 얼마 정도의 간격으로 Pull을 수행할까? Source와 다른 Operator를 Chaining하면 Kafka로부터 데이터를 Pull할 때 해당 Operator Chain이 Blocking 될까?(즉, I/O와 Computing이 합쳐져 있을까, 분리되어 있을까?에 대한 질문) Kafka Transaction이나 발..." }, { "title": "싱글톤 패턴", "url": "/posts/singleton/", "categories": "", "tags": "designpattern", "date": "2021-04-11 13:00:00 +0900", "snippet": "싱글톤 패턴싱글톤 패턴은 프로세스(Java에서는 JVM) 내에 1개의 클래스 인스턴스만을 갖도록 보장하고, 이에 대한 전역 접근점을 제공하는 패턴이다.단순하게 구현하기(Eager Initialization Singleton)싱글톤 패턴을 가장 단순하게 구현하는 방법은 외부에서 해당 클래스의 인스턴스를 생성할 수 없도록 private 생성자를 만들고, 1개의 인스턴스만을 초기화(public static final)하여 이에 대한 접근을 제공하는 방식이다.public class SimpleSingleton { //public s..." }, { "title": "Flink Concept - pipeline.object_reuse", "url": "/posts/flink_object_reuse/", "categories": "flink", "tags": "", "date": "2021-03-27 00:10:00 +0900", "snippet": "개요이전 글에서 pipeline.object_reuse 옵션에 대해 설명하였다. 이 옵션을 사용하면 안되는 경우에 대해서는 Flink 공식 문서에 나와 있지 않으며, 이전 글에서도 제대로 설명하지 못했다.이번 글에서는 언제 이 옵션을 사용하면 안되는지와 그럼에도 불구하고 옵션을 적용하고 싶은 경우 우회하는 방법에 대해 알아보도록 한다.pipeline.object_reuse를 사용하면 안되는 경우pipeline.object_reuse를 사용하면 안되는 경우는 같은 Operator Chain에서 한 Operator의 출력에 두 개..." }, { "title": "LEAD, LAG 함수", "url": "/posts/lead_lag_function/", "categories": "", "tags": "sql", "date": "2021-03-26 23:00:00 +0900", "snippet": "개요데이터 분석 중 사용자의 로그들을 묶어서 봐야 하는 경우가 생겼다. 예를 들자면 아래와 같다.로그 테이블사용자(user)와 방문일(visit_date)로 구성된 테이블이다. user visit_date A 2021-03-01 A 2021-03-04 A 2021-03-08 B 2021-03-24 내가 만들고 싶은 테이블사용자(user)와 방문일(visit..." }, { "title": "Flink Concept - Operator 간 데이터 교환", "url": "/posts/flink_output/", "categories": "flink", "tags": "", "date": "2021-03-26 00:10:00 +0900", "snippet": "개요Flink Job의 Operator 간의 데이터 교환이 어떻게 이루어지는지 알아보고, 유의해야 할 옵션은 어떤 것이 있는지 알아보도록 한다.Flink의 Input과 Ouptut위와 같이 3개의 Operator가 연결되어 있다고 생각해보자. 한 Operator의 출력 데이터는 다른 Operator의 입력 데이터로 사용된다. 그래프 관점에서는 Operator는 Vertex, Operator 사이의 단방향 화살표를 Edge로 표현한다.Flink에서는 Operator(Vertex)를 Input, 데이터 교환 채널(Edge)을 Ou..." }, { "title": "Kerberos 개념", "url": "/posts/kerberos_concept/", "categories": "", "tags": "kerberos", "date": "2021-03-21 19:00:00 +0900", "snippet": " 이 내용은 Wikipedia - Kerberos를 참고하여 작성하였습니다.KerberosKerberos는 보안되지 않은(un-secured) 네트워크에서 안전한 방법(secured manner)으로 노드 간 자격 증명을 수행하는 네트워크 인증 프로토콜이다.사용자 비밀번호가 Network를 통해 교환되지 않기 때문에 MIMT(Main in the middle attack)을 통해 비밀번호 탈취가 불가능한 구조를 가지고 있는데, 어떻게 이런 특징을 가질 수 있는지 알아보도록 한다.참여 주체Kerberos 인증에는 Client,..." }, { "title": "Flink Concept - Checkpointing(1)", "url": "/posts/flink_checkpoint_1/", "categories": "flink", "tags": "", "date": "2021-03-01 17:00:00 +0900", "snippet": " 이 글은 Coursera - Cloud Computing Concepts, Part 1을 참고하여 작성하였습니다.개요스트리밍 애플리케이션은 매우 긴 시간 동안 수행되며 내부적으로 상태를 가지고 있다. 예를 들어 아래와 같이 (사용자, 페이지 체류 시간)이 입력으로 들어오고, 평균 사용자 체류 시간을 계산하는 애플리케이션이 있다고 가정하자.Source -&amp;gt; Task(평균 사용자 체류 시간 계산) Source: 입력 소스로부터 데이터를 가져온다. Task: 사용자 수와 체류 시간을 누적(In-Memory)하며 ..." }, { "title": "Closure란?", "url": "/posts/closure/", "categories": "", "tags": "fp", "date": "2021-01-21 23:00:00 +0900", "snippet": "Closure보다 먼저 알아야할 개념들Lexical scope vs Dynamic scopeLexical scope를 사용하는 언어(C, C++, Java, Javascript 등)에서는 소스코드 내에서 변수나 함수가 정의된 위치와 Lexical Context를 기준으로 이름 참조(Name resolution)를 수행한다. 찾아야하는 이름이 내부 Lexical context에 존재하지 않는 경우 외부 Lexical context에서 이를 찾고, 모든 외부 Lexical context에도 해당 이름이 존재하지 않는 경우 오류가 ..." }, { "title": "FoldLeft와 FoldRight 제대로 알고 사용하기", "url": "/posts/fold_left_right/", "categories": "", "tags": "fp", "date": "2021-01-07 00:40:00 +0900", "snippet": "FoldFold는 주어진 결합 함수(Combining Operation)를 데이터 구조에 대해 재귀적으로 호출하여 결과 값을 만들어내는 고계 함수(Higher-Order Function)이다.잠시 List에 대해서 생각해보자. List는 아래와 같이 두 개로 분류할 수 있다. 빈 List(보통 Nil이라고 부르며, []로 표현한다. Initial Value로도 사용될 수 있다) Prefix 역할을 하는 Element가 다른 List와 결합된 List위 두 분류를 결합하면 List를 재귀적으로 표현할 수 있다.예를 들어 [1..." }, { "title": "Flink Concept - Operator, Task, Parallelism", "url": "/posts/flink_concepts_1/", "categories": "flink", "tags": "", "date": "2020-08-02 18:10:00 +0900", "snippet": "개요진행 중인 프로젝트에서 Flink를 사용할 기회가 생겼다. 처음 코드를 작성할 때는 ‘Spark과 거의 비슷하네?’ 라는 생각을 했는데, 사용하면 할 수록 다른 부분을 많이 느끼게 되어 정리 차 글을 작성한다.아래 자료들을 참고하여 작성하였다. Apache Flink - Flink Architecture 삼성 SDS - 연산 처리의 성능 한계에 도전하는 병렬 컴퓨팅 Streaming Processing with Apache Flink - O’REILLY ZDNET - Understanding task and data..." }, { "title": "Java NIO - 2. Buffers", "url": "/posts/java_nio_buffer/", "categories": "", "tags": "java", "date": "2020-05-24 22:30:00 +0900", "snippet": " Java NIO - 1. 왜 자바의 IO 패키지는 느린가? Java NIO - 2. Buffers개요Java NIO - 1. 왜 자바의 IO 패키지는 느린가?에 이어 Java NIO에서 도입된 Buffer에 대해 알아보고자 한다.BuffersBuffer 객체는 고정 크기의 데이터를 담는 컨테이너이다.각 데이터 타입(Primitive data types)에는 이와 대응하는 Buffer 클래스가 존재한다. 이 Buffer들은 외부적으로는 각 데이터 타입에 대응하는 것으로 보이지만, 내부적으로는 byte 타입에 종속되어 있다...." }, { "title": "Docker(compose)로 Kafka Cluster 실행하기", "url": "/posts/kafka_docker/", "categories": "", "tags": "kafka, docker", "date": "2020-05-17 22:10:00 +0900", "snippet": "개요Kafka 공부 중 Replication 등을 테스트하기 위해 Kafka를 3대로 구성하여 Kafka Cluster를 띄워야 했다. 로컬 환경에 포트와 데이터 경로만 바꿔 실행할 수도 있었지만, 실행/종료가 귀찮을 것이 분명했기 때문에 Kafka를 Docker Image로 만든 뒤 Docker Compose를 통해 띄울 수 있도록 만들어 보았다.처음에는 엄청 쉽게 될 것이라 생각했는데, 생각지도 못한 곳에서 삽질을 하게 되어 그 기록을 정리한다.Kafka Dockerfile 작성하기기본 설정Ubuntu 16.04 이미지를 ..." }, { "title": "Airflow 기본 개념 - DAG와 Operator", "url": "/posts/airflow_concept_dag_operator/", "categories": "", "tags": "airflow", "date": "2020-05-16 23:00:00 +0900", "snippet": " Airflow 튜토리얼 실행해보기 Airflow 기본 개념 - DAG와 Operator개요Airflow에서 사용되는 개념들을 살펴본다. Airflow concepts 페이지를 참고하여 작성하였다.개념DAGDAG(Directed Acyclic Graph)는 관계와 의존성을 가진 작업들의 집합이다. DAG는 파이썬 파일로 정의하고, 코드로써 DAG 구조를 나타낸다.DAG는 작업 자체가 어떻게 구성된지와는 관계 없이 제시간에 정확한 순서로 이슈 없이작업을 실행시키는데에 책임이 있다.DAG를 정의한 파이썬 파일은 Airfl..." }, { "title": "Airflow 튜토리얼 실행해보기", "url": "/posts/airflow_install_and_tutorial/", "categories": "", "tags": "docker, airflow", "date": "2020-05-16 14:30:00 +0900", "snippet": " Airflow 튜토리얼 실행해보기 Airflow 기본 개념 - DAG와 Operator개요Airflow를 사용할 일이 많아질 것 같아 사용법 정리 포스팅을 진행한다.Ubuntu 환경에서 설치, 운영, 활용 실습을 진행하려 했으나, 시간도 넉넉하지 않고 Airflow 자체를 설치하거나 운영하기보다는 활용하는 법을 손에 익히는 일이 중요할 것 같아 Docker airflow라는 Github에 올라와 있는 Docker compose 파일을 사용하여 실습을 진행한다.Airflow 실행미리 준비해야할 사항Docker Image로 ..." }, { "title": "Java NIO - 1. 왜 자바의 IO 패키지는 느린가?", "url": "/posts/java_nio_why_java_io_slow/", "categories": "", "tags": "java", "date": "2020-05-10 20:55:00 +0900", "snippet": " Java NIO - 1. 왜 자바의 IO 패키지는 느린가? Java NIO - 2. Buffers개요요즘 Druid, Kafka 등의 오픈소스를 보면 Java NIO를 사용하는 프로젝트들이 참 많다.Java NIO가 나온지도 오래되었는데 학부 때는 기존 IO 패키지에 있는 것만 배웠고, 업무 중에는 사실 I/O 관련 패키지를 직접 사용하는 경우가 드물다. 그럼에도 불구하고 왜 오픈소스들이 NIO를 사용하는지, NIO를 통해 얻을 수 있는 것은 무엇인지 정리하기 위해 관련 서적과 문서를 통해 Java의 NIO 공부를 진행하..." }, { "title": "LeetCode Weekly Contest 188", "url": "/posts/leetcode_contest_188/", "categories": "", "tags": "ps, leetcode", "date": "2020-05-10 19:00:00 +0900", "snippet": "참여 후기오늘은 Mock이 아니라 개별 문제를 풀었다. 개인 사정에 컨디션이 별로인 상태라서 Mock으로 Contest를 진행해버리면 더 Depressed 될 것 같아서..Weekly Contest 1881441. Build an Array With Stack Operations문제 설명목표 값이 저장되어 있는 배열 target과 정수 n이 주어진다.1 ~ n의 정수를 Push/Pop 연산을 이용하여 target을 만들기 위해 어떻게 Push와 Pop을 배치해야하는지 반환해야 한다.접근 방식1 ~ n까지 순회하며 현재 숫자가 ..." }, { "title": "LeetCode Biweekly Contest 25", "url": "/posts/leetcode_biweekly_contest_25/", "categories": "", "tags": "ps, leetcode", "date": "2020-05-05 19:50:00 +0900", "snippet": "참여 결과 및 후기 Rank Score Finish Time Q1 Q2 Q3 Q4 1983 / 5867 12 1:14:28 0:03:23 0:35:04 0:54:28 - 이번 Contest도 Weekly Contest 187과 같이 Mock Contest로 진행하였다.Hard 문제인 Q4는 오늘도 풀지 못했고, Q2에서 문제 조건 때문에 4번이나 제출해서 완전 망..." }, { "title": "LeetCode Weekly Contest 187", "url": "/posts/leetcode_contest_187/", "categories": "", "tags": "ps, leetcode", "date": "2020-05-03 20:00:00 +0900", "snippet": "참여 결과 및 후기 Rank Score Finish Time Q1 Q2 Q3 Q4 860 / 9245 12 8:09:00 7:36:30 7:40:23 8:09:00 - 오전에 일정이 있어 정규 Contest 시간에는 참여하지 못하고 Mock Contest로 참석했다. 어차피 시간이 되었어도 요즘 꾸준히 문제를 풀지 않았었기 때문에 자신이 없어 참여하지는 않았을 것..." }, { "title": "간단한 Kafka Producer를 만들고 동작원리를 알아보자", "url": "/posts/kafka_producer/", "categories": "", "tags": "kafka", "date": "2020-05-03 18:10:00 +0900", "snippet": "Kafka Producer메시지 발행/구독 시스템에서 Producer는 메시지의 발행을 수행하는 컴포넌트이다.이 글에서는 Kafka Client API를 사용하여 Kafka Producer를 만들어보고, 메시지가 실제로 어떤 과정을 거쳐 전달되는지 알아본다.build.gradle에 Kafka Client 의존성 추가하기build.gradle의 dependencies 블록에 아래 구문을 추가하여 Kafka Client API 의존성을 추가한다. 오늘 날짜(2020년 5월 2일) 기준으로 2.5.0 버전(Release: 2020/..." }, { "title": "Kafka의 Topic, Partition, Segment, Message", "url": "/posts/kafka_concept/", "categories": "", "tags": "kafka", "date": "2020-05-02 15:00:00 +0900", "snippet": "TopicTopic은 메시지가 발행되는 카테고리 혹은 Feed로써, 메시지들의 스트림으로 볼 수 있다. 다양한 생산자(Producer)와 소비자(Consumer)들이 Topic에 메시지를 발행하거나, Topic으로부터 발행된 메시지를 가져와 사용한다. Kafka의 문서나 관련 서적에서 Kafka를 분산 커밋 로그(Distributed commit log)라고 부른다. 분산 커밋 로그에서의 로그가 Topic과 일치하는 개념이라고 보면 된다. Kafka 소스코드에는 Topic보다는 Log라는 용어가 더 많이 사용된다.Topic의..." }, { "title": "LeetCode - Palindrome Number", "url": "/posts/PalindromeNumber/", "categories": "", "tags": "ps, leetcode", "date": "2020-04-29 12:57:00 +0900", "snippet": " 출처: LeetCode - Palindrome Number 난이도: 중 관련 기술: Array 풀이일 2020년 04월 29일 문제 내용주어진 정수가 Palindrome인지 아닌지 판별하는 문제이다. 음수는 ‘-‘ 부호 때문에 절대 Palindrome이 될 수 없다는 것이 포인트인듯 하다.mod 연산을 통한 풀이주어진 정수를 10으로 나눈 나머지를 리스트에 저장하고, 이 리스트에 저장된 1 ~ (N/2) - 1번째 값과 N ~ (N/2)까지의 값이 같은지 확인하면 된다.class Solutio..." }, { "title": "LeetCode - Combination Sum II", "url": "/posts/CombinationSumII/", "categories": "", "tags": "ps, leetcode", "date": "2020-04-29 12:37:00 +0900", "snippet": " 출처: LeetCode - Combination Sum II 난이도: 중 관련 기술: Array, Back Tracking 풀이일 2020년 04월 29일 문제 내용주어진 배열의 원소를 사용하여 만든 배열의 합이 주어진 목표 값이 되는 배열들을 만들어 반환하는 문제이다.LeetCode - Combination Sum과 거의 동일한 문제인데, 조건이 약간 다르다.Combination Sum에서는 원본 배열에 중복된 요소가 없지만, 각 요소를 여러번 사용해도 되는 반면, Combination Su..." }, { "title": "LeetCode - Combination Sum", "url": "/posts/CombinationSum/", "categories": "", "tags": "ps, leetcode", "date": "2020-04-28 22:00:00 +0900", "snippet": " 출처: LeetCode - Combination Sum 난이도: 중 관련 기술: Array 풀이일 2020년 04월 28일 문제 내용주어진 배열의 원소를 사용하여(중복 사용 가능) 만든 배열의 합이 주어진 목표 값이 되는 배열들을 만들어 반환하는 문제이다. 결과 리스트에는 중복된 배열이 존재하면 안된다.Back Tracking을 이용한 풀이Back Tracking을 이용하여 풀이하면 된다. 다만 원소를 중복 사용할 수 있기 때문에, 다음 함수를 호출하기 전에 자신이 만들 수 있는 경우의 수를 ..." }, { "title": "LeetCode - Find Minimum in Rotated Sorted Array", "url": "/posts/FindMinimuminRotatedSortedArray/", "categories": "", "tags": "ps, leetcode", "date": "2020-04-28 21:45:00 +0900", "snippet": " 출처: LeetCode - Find Minimum in Rotated Sorted Array 난이도: 중 관련 기술: Array 풀이일 2020년 04월 28일 문제 내용오름차순으로 정렬된 배열을 특정 기준점으로 뒤집어놓은 배열이 존재한다(예를 들어 [1, 2, 3, 4]와 같은 배열(Zero-Based)이 있을 때 Index 2를 기준으로 뒤집으면 [4, 1, 2, 3] 이 된다). 이 배열에서 가장 작은 값을 찾는 문제이다.Sliding Window를 이용한 풀이이진 탐색을 이용하여 풀이하..." }, { "title": "LeetCode - Maximum Points You Can Obtain from Cards", "url": "/posts/MaximumPointsYouCanObtainFromCards/", "categories": "", "tags": "ps, leetcode", "date": "2020-04-26 22:30:00 +0900", "snippet": " 출처: LeetCode - Maximum Points You Can Obtain from Cards 난이도: 중 관련 기술: Array 풀이일 2020년 04월 26일 문제 내용임의의 배열과 정수 K가 주어진다. 배열의 양 끝에서 하나씩 K개의 숫자를 뽑아 만들 수 있는 최대 값을 반환하는 문제이다.1차 풀이Recursion을 통해 접근한 뒤, Memoization을 통해 풀이하려 했다.결과는 실패였다. Memoization 시 2차원 배열을 만들었는데 입력 배열의 크기가 100,000까지 늘..." }, { "title": "Druid에서의 Bitmap Index", "url": "/posts/Apache_Druid_bitmap_index/", "categories": "", "tags": "apache-druid", "date": "2020-04-26 17:45:00 +0900", "snippet": "개요Druid에서는 데이터 필터 속도 최적화와 저장 공간 절약을 위해 내부적으로 Bitmap Index를 사용한다.아래와 같은 스키마를 가진 데이터셋(테이블)이 존재한다고 가정해보자. ID: Long Country Code: Integer City Code: Integer특정 조건에 일치하는 데이터를 찾기 위해 우리는 아래와 같은 쿼리들을 주로 수행한다.CountryCode로 ID 조회하기SELECT ID FROM TABLE WHERE CountryCode=3CountryCode와 CityCode를 통해 ID 조회하기SE..." }, { "title": "Druid의 Kafka Indexing Task에서 Roll-up은 어떻게 동작하는가?", "url": "/posts/Druid_Kafka_Indexing_Rollup/", "categories": "", "tags": "apache-druid", "date": "2020-04-19 21:58:00 +0900", "snippet": "Roll-upDruid에서는 입수(Ingestion) 단계에서 데이터를 선집계(Pre-Aggregate)하는 Roll-up이라는 기능을 제공한다. Roll-up을 구성할 경우 모든 Dimension의 값이 동일한 데이터들의 Metric들에 대해 입수 단계에서 sum, count 등의 집계를 미리 수행하게 된다.이 글에서는 Apache Druid - Tutorial: Roll-up를 따라할 때 Peon 프로세스 내에서 어떤 일이 일어나는지 확인해본다.Segment와 IndexRoll-up 과정을 알기 위해서는 Druid 내의 데..." }, { "title": "자바의 클래스로더 알아보기", "url": "/posts/java_class_loader/", "categories": "", "tags": "java", "date": "2020-04-18 15:00:00 +0900", "snippet": " 다음 자료들을 참고하여 작성하였습니다. Geeksforgeeks - Classloader in Java Baeldung - Java Classloaders 또한 아래 내용들은 JAVA8 기준으로 작성되었습니다.클래스로더자바 클래스들은 시작 시 한번에 로드되지 않고, 애플리케이션에서 필요할 때 로드된다. 클래스 로더는 JRE의 일부로써 런타임에 클래스를 동적으로 JVM에 로드 하는 역할을 수행하는 모듈이다. 자바의 클래스들은 자바 프로세스가 새로 초기화되면 클래스로더가 차례차례 로딩되며 작동한다.부트스트랩..." }, { "title": "LeetCode Weekly Contest 178", "url": "/posts/leetcode_contest_178/", "categories": "", "tags": "ps, leetcode", "date": "2020-03-01 17:30:00 +0900", "snippet": "참여 결과 및 후기 Rank Score Finish Time Q1 Q2 Q3 Q4 1746 / 9210 12 1:29:37 0:05:14 0:20:06 1:14:37 - 시간 내에 3문제를 풀어 1746등을 기록했다. 4번 문제는 Contest가 끝나고 10분 뒤에 제출했는데 맞춰버렸다…2019년 12월 중순부터 시작해서 매주 참여하고 있는데, 2주 전부터 한 두..." }, { "title": "ThreadPoolExecutor에 대한 오해와 진실", "url": "/posts/truth_of_threadpoolexecutor/", "categories": "", "tags": "java", "date": "2020-02-21 01:00:00 +0900", "snippet": "ThreadPoolExecutor에 대한 오해와 진실회사에서 팀원 분이 코드 리뷰를 해주셨는데, ThreadPoolExecutor을 잘못 사용하고 있다는 내용이었다.내가 작성한 원본 코드는 대략 아래와 같다.int numTasks = 60;CountDownLatch countDownLatch = new CountDownLatch(numTasks);ThreadPoolExecutor threadPoolExecutor= new ThreadPoolExecutor(10, 50, 10, TimeUnit.SECONDS, new Linked..." }, { "title": "Spring Sleuth 사용 시 Thread 간 Trace ID가 공유되지 않는 문제 해결하기", "url": "/posts/spring-sleuth-propagate/", "categories": "", "tags": "spring", "date": "2020-02-17 23:00:00 +0900", "snippet": " 두 줄 요약 ApplicationContext와 BeanFactory는 다른 객체이다. TraceableExecutorService를 초기화할 때는 ApplicationContext가 아니라 BeanFactory를 활용하라.Spring Sleuth 사용 시 Thread 간 Trace ID가 공유되지 않는 문제 해결하기기존에 Spring Sleuth를 사용할 때 Runnable을 구현한 클래스에 Zipkin의 Tracer를 의존성으로 주입받아 run() 메서드 시작 시 Span을 생성하는 로직을 추가하여 사용하였다.클래스..." }, { "title": "Linux - PID, PPID, PGID, SID란?", "url": "/posts/sid_pid_ppid_pgid/", "categories": "", "tags": "linux", "date": "2020-01-02 23:00:00 +0900", "snippet": "Linux의 PID, PPID, PGID, SID란?개념 정리PIDPID(Process ID)는 운영체제에서 프로세스를 식별하기 위해 프로세스에 부여하는 번호를 의미한다.PPIDPPID(Parent Process ID)는 부모 프로세스의 PID를 의미한다.만일 부모 프로세스가 자식 프로세스보다 일찍 종료되는 경우 자식 프로세스는 고아 프로세스가 되어 PPID로 init process의 PID(1)를 가지게 된다.PGID프로세스 그룹(Process Group)은 1개 이상의 프로세스의 묶음을 의미한다.PGID(Process Gr..." }, { "title": "LeetCode - Search Suggestions System", "url": "/posts/SearchSuggestionsSystem/", "categories": "", "tags": "ps, hashtable, string", "date": "2019-12-02 20:30:00 +0900", "snippet": " 출처: LeetCode - Search Suggestions System 난이도: 중 관련 기술: Hashtable, String 문제 요약: 물건의 목록(products)과 검색 문자열(searchWord)가 주어진다. 검색 문자열을 타이핑 할 때 각 순간에서의 문자열을 접두어로 가지는 물건의 목록을 출력하는 문제이다. 단, 출력하는 물건의 목록은 Lexical Order로 3개로 제한한다. 풀이일 2019년 12월 02일 풀이 방법 Product: [“mousepad”, “monitor..." }, { "title": "Apache Hadoop - Fair Scheduler", "url": "/posts/Fair_Scheduler/", "categories": "", "tags": "apache-hadoop, study", "date": "2019-11-16 14:00:00 +0900", "snippet": " 출처 Untangling Apache Hadoop YARN, Part 3: Scheduler Concepts Untangling Apache Hadoop YARN, Part 4: Fair Scheduler Queue BasicsYarn Scheduler단일 컴퓨터는 하나의 Process를 실행 시킬 수 있는 Core를 여러 개 가진 CPU를 가지고 있다. 수 백개의 Process를 동시에 수행할 수 있도록 운영체제에는 스케쥴러가 존재한다.클러스터에서 수행되는 어플리케이션은 서로 다른 노드에서 수행되는 수십 개의 작업으로..." }, { "title": "Apache Druid - Segments", "url": "/posts/Apache_Druid-Segments/", "categories": "", "tags": "apache-druid, study", "date": "2019-11-10 15:00:00 +0900", "snippet": " 스터디를 위해 Apache Druid 공식 문서를 번역/요약한 문서입니다.SegmentsApache Druid는 시간 기준으로 파티셔닝된 색인을 Segment 파일에 저장한다. 하나의 Segment 파일은 Ingestion 단계에서 정의한 granularitySpec의 segmentGranulairty 만큼의 Time Interval을 기준으로 생성된다.Druid가 Heavy Query를 효율적으로 수행하게 하기 위해서는 Segment 파일의 크기가 300mb ~ 700mb 정도로 생성되도록 구성하는 것을 추천한다. ㄷ" }, { "title": "Apache Spark - Detected implicit cartesian product for INNER join between logical plans", "url": "/posts/spark-implicit-caresian/", "categories": "", "tags": "apache-spark", "date": "2019-11-09 23:00:00 +0900", "snippet": " Spark에서의 Join 상황에서 발생할 수 있는 “Detected implicit cartesian product for INNER join between logical plans” 이슈에 대해서 정리하였습니다.문제 상황다음과 같은 두 개의 테이블이 존재한다고 가정하자.사용자(users) idx name age 1 user1 20 2 user2 21 3 user3 ..." }, { "title": "LeetCode - Find All Anagrams in a String", "url": "/posts/FindAllAnagramsInString/", "categories": "", "tags": "ps, hashtable, sliding window", "date": "2019-11-06 21:30:00 +0900", "snippet": " 출처: LeetCode - Find All Anagrams in a String 난이도: 중 관련 기술: Hashtable, Sliding Window 문제 요약: 문자열 S, P가 주어질 때 S 내에 존재하는 P의 아나그램을 찾는 문제이다. 풀이일 2019년 11월 06일 풀이 방법Anagram은 단어나 문장을 구성하고 있는 문자의 순서를 바꾸어 다른 단어나 문장을 만드는 것이다. 즉, 두 문장이 있을 때 문장을 이루는 각 문자의 갯수만 동일하면 두 문장은 아나그램으로 볼 수 있다.예를 들..." }, { "title": "Apache Druid - Design", "url": "/posts/Apache_Druid_Design/", "categories": "", "tags": "apache-druid, study", "date": "2019-11-05 10:00:00 +0900", "snippet": " 스터디를 위해 Apache Druid 공식 문서를 번역/요약한 문서입니다.Design 출처: Apache druid - architectureDruid는 Multi process로 구성된 분산 아키텍쳐를 가지고 있다. 각 Process는 독립적으로 구성/확장될 수 있어 클러스터에 맞게 탄력적으로 운영할 수 있다. 이러한 설계는 하나의 컴포넌트가 일시적으로 사용 불가능한 상태가 되더라도, 다른 컴포넌트에 영향을 미치지 않기 때문에 Fault-tolerance한 특성도 가지고 있다.Processes and ServersDru..." }, { "title": "Apache Spark - Data Source V2(1)", "url": "/posts/spark-datasource-v2-1/", "categories": "", "tags": "apache-spark", "date": "2019-11-01 22:00:00 +0900", "snippet": " Apache Spark Data Source V2 with Wehchen Fan and Gengliang Wang Github - spirom/spark-datasources 을 참고하여 작성하였습니다.Data Source API V2What is Data Source API?Storage System에(서) 어떻게 데이터를 쓰고 읽는지 정의하는 API이다. 예를 들어 Hadoop는 InputFormat/OutputFormat, Hive는 Serde, Presto는 Connector 등의 Data Sour..." }, { "title": "LeetCode - Permutations II", "url": "/posts/permutations_2/", "categories": "", "tags": "ps, array, backtracking", "date": "2019-10-29 19:30:00 +0900", "snippet": " 출처: LeetCode - Longest Common Prefix 난이도: 하 관련 기술: String 문제 요약: 입력 문자열들의 가장 긴 Prefix를 찾는 문제이다. 풀이일 2019년 10월 8일 풀이 방법단순한 for문을 통해 풀 수 있는 문제이다. 가장 짧은 문자열의 길이를 찾는다.(모든 문자열의 Prefix는 가장 짧은 문자열의 길이를 넘어설 수 없기 때문) 모든 문자열의 0 ~ 가장 짧은 문자열의 길이 - 1 까지의 값들을 비교하며 다른 문자가 1개라도 나올 때 중지한다.코드..." }, { "title": "Apache Druid - Getting started", "url": "/posts/Apache_Druid_Getting_Started/", "categories": "", "tags": "apache-druid, study", "date": "2019-10-26 22:00:00 +0900", "snippet": " 스터디를 위해 Apache Druid 공식 문서를 번역/요약한 문서입니다.Getting startedIntroduction to Apache DruidDruid란?Apache Druid는 대규모 데이터를 대상으로 slice-and-dice 분석을 지원하기 위해 만들어진 실시간 분석용 데이터베이스이다. slice and dice? 정보를 작은 단위로 쪼개어 다각도에서 바라볼 수 있도록 하는 것 출처: What is slice and dice?Druid의 코어 아키텍쳐는 Data Warehouse, Time Series D..." }, { "title": "LeetCode - Longest Common Prefix", "url": "/posts/LongestCommonPrefix/", "categories": "", "tags": "ps, string", "date": "2019-10-08 21:00:00 +0900", "snippet": " 출처: LeetCode - Longest Common Prefix 난이도: 하 관련 기술: String 문제 요약: 입력 문자열들의 가장 긴 Prefix를 찾는 문제이다. 풀이일 2019년 10월 8일 풀이 방법단순한 for문을 통해 풀 수 있는 문제이다. 가장 짧은 문자열의 길이를 찾는다.(모든 문자열의 Prefix는 가장 짧은 문자열의 길이를 넘어설 수 없기 때문) 모든 문자열의 0 ~ 가장 짧은 문자열의 길이 - 1 까지의 값들을 비교하며 다른 문자가 1개라도 나올 때 중지한다.코드..." }, { "title": "LeetCode - K diff pairs in array", "url": "/posts/K-Diff-Pairs/", "categories": "", "tags": "ps, twopointer", "date": "2019-10-08 21:00:00 +0900", "snippet": " 출처: LeetCode - K diff pairs in array 난이도: 하 관련 기술: Two Pointer, Sort 문제 요약: 배열 내의 임의의 두 숫자의 조합의 차가 k인 조합의 갯수를 찾는 문제이다. 풀이일 2019년 10월 8일 풀이 방법정렬과 투포인터를 이용하여 풀 수 있는 문제이다. 정렬을 수행한다. 정렬이 이루어진 배열은 n-k번째 요소는 반드시 n번째 요소보다 작거나 같은 값을 가지게 된다. 투포인터 알고리즘을 이용하여 차이가 2인 조합들을 찾아 카운팅한다. 이 때 ..." }, { "title": "LeetCode - Generate Parentheses", "url": "/posts/GenerateParentheses/", "categories": "", "tags": "ps, recursion", "date": "2019-10-08 21:00:00 +0900", "snippet": " 출처: LeetCode - Generate Parentheses 난이도: 하 관련 기술: Recursion 문제 요약: n개의 (, )를 이용하여 완전히 닫힌 괄호 조합을 만들어내는 문제 풀이일 2019년 10월 8일 풀이 방법재귀 함수를 이용하여 풀이할 수 있는 문제이다.닫힌 괄호를 만들어내야 하는데, ‘(‘, ‘)’를 기존 문자열에 붙일 때 기존 문자열에서의 ‘(‘의 갯수는 ‘)’보다 같거나 많아야 한다. 또한 기존 문자열의 ‘(‘와 ‘)’의 갯수가 같은 경우 반드시 ‘(‘를 붙여주어야 ..." }, { "title": "LeetCode - Super Reduced String", "url": "/posts/SuperReducedString/", "categories": "", "tags": "ps, string, recursion", "date": "2019-10-07 21:40:00 +0900", "snippet": " 출처: HackerRank - Super Reduced String 난이도: 하 관련 기술: String, Recursion 문제 요약: 입력으로 주어진 문자열 내의 연속으로 2번 등장하는 문자를 제거하는 문제이다. 예를들어 aaabb가 나온다면 aaabb -&amp;gt; abb -&amp;gt; a가 된다. 풀이일 2019년 10월 7일 풀이 방법재귀 함수를 만들어 풀이하면 된다. 각 Step에서는 문자열을 순회하며 연속 2회 등장하는 문자를 확인한다. 2회 연속 등장하는 문자가 발생한..." }, { "title": "LeetCode - Median of Two Sorted Arrays", "url": "/posts/MedianOfTwoSortedArrays/", "categories": "", "tags": "ps, array", "date": "2019-10-07 05:00:00 +0900", "snippet": " 출처: LeetCode - Median of Two Sorted Arrays 난이도: 상 관련 기술: Array, Binary Search, Divide and Conquer 문제 요약: 미리 정렬된 2개의 배열을 합친 배열의 중간 값(median)을 찾아내면 되는 문제이다. 풀이일 2019년 10월 7일 풀이 방법입력으로 들어온 2개의 배열(nums1, nums2)이 이미 정렬되어 있는 상태이므로, 2개의 포인터(l = nums1의 요소를 가리킴, r = nums2의 요소를 가리킴)을 두고..." }, { "title": "LeetCode - Longest Substring Without Repeating Characters", "url": "/posts/LongestSubstringWithoutRepeatingCharacters/", "categories": "", "tags": "ps, string, slidingwindow", "date": "2019-10-07 04:00:00 +0900", "snippet": " 출처: LeetCode - Longest Substring Without Repeating Characters 난이도: 중 관련 기술: Sliding Window 문제 요약: 입력으로 주어진 문자열(s)에서 반복된 문자를 가지지 않는 최대 길이의 부분 문자열을 찾는 문제이다. 풀이일 2019년 10월 7일 풀이 방법s(l, r)이 반복된 문자를 가지지 않고, s(r+1)이 s(l, r)에 포함되지 않는 경우 s(l, r+1) 또한 반복된 문자를 가지지 않는 최대 길이 문자열로 볼 수 있다.다..." }, { "title": "Spark RDD의 count()는 어떻게 동작하는가?(Shuffle이 없는, Driver 편)", "url": "/posts/how-rdd-count-works/", "categories": "", "tags": "apache-spark", "date": "2019-07-21 21:00:00 +0900", "snippet": "Spark RDD의 count()는 어떻게 동작하는가?(Shuffle이 없는, Driver 편)Spark RDD의 기본 연산 중 하나인 count()가 어떻게 동작하는지 알아보도록 한다.Shuffle이 들어가면 분석이 너무 어렵기 때문에 Shuffle이 발생하지 않는 코드로만 추적해보았으며, 이번 글에서는 Driver에서 발생하는 과정만을 다룬다.Shuffle이 없는 count()Spark 코드를 확인해보면 Shuffle이 존재하는 연산보다 Shuffle이 존재하지 않는 연산이 훨씬 쉽고, 읽어야 할 코드의 양이 적기 때문에 ..." }, { "title": "Validation에 책임 연쇄 패턴 적용하기", "url": "/posts/validation-with-chain-of-responsibility/", "categories": "", "tags": "spring", "date": "2019-06-29 22:00:00 +0900", "snippet": "Validation에 책임 연쇄 패턴 적용하기데이터를 저장하기 전에 데이터에 대한 검증(Validation)을 수행해야 하는 경우가 있다.예전에는 아래와 같이 데이터를 관리하는 클래스 내에 validation 이라는 메서드를 정의해서 기본 validation을 수행하고, 필요한 경우 해당 메서드를 재정의하여 사용하거나 preValidate, postValidate와 같은 추상 메서드를 만들어놓고 하위 클래스에서 이를 구현하면, validate 함수 내에서 이를 호출해주는 방식으로 구현했었다.OldValidationExample..." }, { "title": "Streaming Systems - Streaming 102(1)", "url": "/posts/streaming-102/", "categories": "", "tags": "streaming", "date": "2019-06-27 22:00:00 +0900", "snippet": "Streaming 102Streaming 101에서 등장한 개념 이외에도 Trigger, Watermark, Accumulation이라는 개념이 등장한다.TriggerWindow의 Output을 언제 내보낼지 결정하는 동작을 의미한다.단순히 한번만 Window의 결과를 출력하지 않고 Window의 결과물이 달라짐에 따라 여러 번 결과를 출력하는 것도 가능하다. 이렇게 동일한 Window의 결과를 여러 번 출력하는 것은 Late data에 의해 변경되는 Window의 결과를 출력하는데 효과적이다.WatermarkWatermark..." }, { "title": "Streaming Systems - Streaming 101", "url": "/posts/streaming-101/", "categories": "", "tags": "streaming", "date": "2019-06-22 15:00:00 +0900", "snippet": "Streaming 101스트리밍 데이터 처리는 다음과 같은 장점을 가지고 있다. 배치 시스템에 비해 낮은 레이턴시를 보장할 수 있는 방법이다. 무한한 크기의 데이터를 처리하는 시스템을 통해 거대하고 Unbound 된 데이터를 처리할 수 있다. 데이터를 처리 워크로드를 분산시킬 수 있다.스트리밍 시스템은 무한한 데이터를 처리할 수 있도록 디자인된 데이터 처리 엔진이다.데이터를 분류하는 기준은 크게 두 가지가 존재하는데, Cardinality와 Constitution이다.Cardinality데이터의 크기를 의미한다. Card..." }, { "title": "Spring IoC Container - Container, Bean overview", "url": "/posts/spring-core-1/", "categories": "", "tags": "spring", "date": "2019-05-18 10:00:00 +0900", "snippet": "The IoC ContainerIntroduction to the Spring IoC Container and BeansInversion of Control(IoC, 제어의 역전)은 Dependency Injection(DI, 의존성 주입)로도 알려져 있다.객체는 자신이 동작하는데에 필요한 의존성(객체)을 생성자 팩토리 메서드의 인자 Setter를 통해 설정할 수 있도록 한다.Container는 객체를 생성할 때 의존성들을 위에서 정의한 방법으로 주입해준다.객체가 자신의 초기화나 의존성의 설정을 스스로 하는 것이 아니라 ..." }, { "title": "Spring + MongoDB + Docker 조합 사용 테스트", "url": "/posts/spring-with-docker-1/", "categories": "", "tags": "docker, spring", "date": "2019-05-06 10:00:00 +0900", "snippet": "프로젝트 초기화git 초기화 Git 페이지에서 spring_mongodb_docker Repository를 초기화한다. git pull https://github.com/leeyh0216/spring_mongodb_docker.git 명령어를 통해 로컬로 Clone 한다. gitignore.io 페이지에서 gradle, java, intellij로 초기화한 .gitignore을 디렉토리에 추가한다. Spring Project 초기화 최상위 디렉토리 아래에 spring-boot-tes..." }, { "title": "Spring Core Technologies - Annotation-based container configuration(1)", "url": "/posts/annotation-based-container-configuration/", "categories": "", "tags": "spring", "date": "2019-01-28 10:00:00 +0900", "snippet": "Annotation-based container configuration어노테이션 방식의 설정이 XML 방식의 설정보다 나은가?어노테이션 방식과 XML 방식은 각각 장/단점이 있기 때문에, 어느 것이 낫다고 말할 수는 없다.어노테이션 방식의 경우 명료하고 정확한 설정을 할 수 있도록 선언 내부에 많은 컨텍스트를 포함하고 있다는 장점이 있다.반면 XML 방식의 경우 소스코드의 수정이나 재컴파일 없이도 XML만으로 프로그램의 동작을 제어할 수 있는 장점이 있다.몇몇 개발자들은 소스 코드 내에 객체 간의 연결을 가져가는 것을 선호하..." }, { "title": "Spring Core Technologies - Customizing the nature of a bean", "url": "/posts/spring-customizing-the-nature-of-a-bean/", "categories": "", "tags": "spring", "date": "2019-01-23 10:00:00 +0900", "snippet": "Customizing the nature of a beanLifecycle callbacksSpring에서 제공하는 InitializingBean 혹은 DisposableBean 인터페이스를 구현한다면, Bean의 Lifecycle을 Container에게 위임할 수 있다.Container는 Bean 생성 과정에서는 afterPropertiesSet 함수를 호출하고 Bean의 소멸 과정에서는 destroy 함수를 호출한다. InitializingBean과 DisposableBean은 Spring Framework에서 제공하는 ..." }, { "title": "Spring Core Technologies - Bean Scopes", "url": "/posts/spring-bean-scopes/", "categories": "", "tags": "spring", "date": "2019-01-22 10:00:00 +0900", "snippet": "Bean ScopesBean Definition을 만든다는 것은, Bean으로 생성할 클래스를 통해 어떻게 객체를 만들어 내는지에 대한 방법(Recipe)을 만들어 내는 것이다.Bean Definition에는 생성할 Bean의 의존성(Dependency) 설정값(Configuration values) Scope이 포함된다.설정을 통해 객체의 Scope을 지정하는 방식은 자바의 클래스 레벨에서 Scope을 제어하는 것보다 강력하고 유연하다.Spring에서는 7개의 Scope을 지원하며, Non Web Application에..." }, { "title": "Spring with RabbitMQ(2)", "url": "/posts/spring-rabbitmq-2/", "categories": "", "tags": "spring, rabbitmq", "date": "2018-12-30 10:00:00 +0900", "snippet": "Spring with RabbitMQ RabbitMQ에서 사용하는 AMQP 0-9-1 모델과 RabbitMQ의 컨셉을 정리하기 위해 작성하였다.Spring과의 자세한 매핑은 이후 글에서 작성할 예정AMQP 0-9-1 ModelHigh-level Overview of AMQP 0-9-1 and the AMQP ModelWhat is AMQP 0-9-1?AMQP 0-9-1(Advanced Message Queueing Protocol)은 클라이언트 어플리케이션들이 미들웨어 메시지 브로커를 통해 통신할 수 있도록 하는 메시지 프로..." }, { "title": "Spring with RabbitMQ(1)", "url": "/posts/spring-rabbitmq-1/", "categories": "", "tags": "spring, rabbitmq", "date": "2018-12-30 10:00:00 +0900", "snippet": "Spring with RabbitMQPre Requirements RabbitMQ 3.6RabbitMQ 설치(Docker)RabbitMQ를 물리 서버에 설치하기 위해서는 Erlang 설치를 선행한 후 RabbitMQ를 설치해야 하지만 테스트 용도이기 때문에 Docker로 설치 진행한다.Docker Hub의 RabbitMQ 페이지를 참고하여 설치.이미지 Pulldocker pull rabbitmq 명령어를 이용하여 이미지 다운로드. 최신 버전이 아닌 3.6 버전을 사용할 예정이므로 docker pull rabbitmq:3.6-..." }, { "title": "Spring Core Technologies - The IoC Container(4)", "url": "/posts/spring-core-4/", "categories": "", "tags": "spring", "date": "2018-12-23 10:00:00 +0900", "snippet": "The IoC ContainerDependencies간단한 어플리케이션부터 기업형 어플리케이션까지 하나의 객체로만 동작하는 프로그램은 없다. 적어도 몇개의 객체들이 서로 상호작용하며 어플리케이션을 구성하고 있다.Dependency Injection의존성 주입(Dependency Injection, D.I)은 객체들이 자신의 의존성(의존 객체)을 생성자 인자 팩토리 메서드의 인자 Setter를 통해 정의하여 Container가 해당 객체를 생성할 때, 필요한 의존 객체들을 주입해주는 방식을 말한다.의존성 주입을 이용하면 코..." }, { "title": "Inversion of Control Containers and the Dependency Injection pattern", "url": "/posts/ioc_and_di_pattern/", "categories": "", "tags": "spring", "date": "2018-12-19 10:00:00 +0900", "snippet": " 이 글은 Martin Fowler의 Inversion of Control Containers and the Dependency Injection pattern을 요약 정리한 글입니다.Inversion of Control Containers and the Dependency Injection pattern많은 오픈소스들은 J2EE 기술에 대한 대안을 구축하는 다양한 활동을 하고 있다. 이러한 활동은 J2EE의 복잡도를 획기적인 방법으로 낮추기 위함이다.이들이 다루는 공통적인 이슈는 서로 다른 요소들을 어떻게 결합하는지에 대한..." }, { "title": "Spring Core Technologies - The IoC Container(3)", "url": "/posts/spring-core-3/", "categories": "spring", "tags": "", "date": "2018-12-03 10:00:00 +0900", "snippet": "The IoC ContainerIntroduction to the Spring IoC Container and BeansBean DependenciesApplication에 Service Layer 역할을 하는 MyService와 Persistent Layer 역할을 하는 MyRepository 클래스가 있다고 가정해보자.MyService 클래스가 동작하기 위해서는 MyRepository 객체가 필요하다.(즉, MyService 클래스가 MyRepository 클래스에 의존적이다)이러한 경우 아래와 같은 코드를 작성해야 할까?..." }, { "title": "Spring Core Technologies - The IoC Container(2)", "url": "/posts/spring-core-2/", "categories": "spring", "tags": "", "date": "2018-11-29 21:00:00 +0900", "snippet": "The IoC ContainerIntroduction to the Spring IoC Container and BeansBean 선언 시의 Interface 활용Bean 객체를 초기화하여 반환하는 메소드(@Bean 어노테이션이 붙은) 만들어 ApplicationContext에서 찾아 사용하는 예제를 이전 글에서 만들어 보았다.해당 예제에서는 초기화하여 반환하는 객체의 타입과 반환 타입이 완전히 일치했는데, 반환 타입은 구체화된 클래스가 아닌 Interface 혹은 Abstract Class로 설정할 수 있다.먼저 아래와 같이 ..." }, { "title": "ElasticSearch + MetricBeat + Kibana로 서버 모니터링하기", "url": "/posts/elasticsearch-1/", "categories": "elasticsearch", "tags": "", "date": "2018-11-19 15:00:00 +0900", "snippet": "개요내가 근무하는 팀에서의 프로젝트는 아래와 같이 크게 두 가지로 분류된다. Hadoop Cluster에서 동작하는 배치 작업(일, 시간 단위) 위의 배치 작업의 메타데이터 및 작업 상태, 의존성 등을 관리하는 웹 서비스(WAS)Hadoop Cluster의 경우 다른 팀에서 운영을 맡고 있기 때문에 내가 작성한 프로그램이 사용하는 리소스나 코드를 최적화 시켜주면 운영 과정에서는 별다른 문제가 발생하지 않는다.그러나 웹 서비스의 경우 동작하는 서버를 우리 팀에서 관리하기 때문에 어플리케이션 뿐만 아니라 서버, DB 또한 우리..." }, { "title": "Spark UDF와 DataSet에서의 NULL 처리", "url": "/posts/spark-udf-null/", "categories": "", "tags": "apache-spark", "date": "2018-11-13 10:00:00 +0900", "snippet": "개요Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다.SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다.다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다.UDF에서의 NULL 처리String TypeString 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다.주어진 문자열을 시작 위치부터 2만큼 잘라내..." }, { "title": "Spring Cloud - Zuul(1)", "url": "/posts/spring-cloud-zuul/", "categories": "", "tags": "spring", "date": "2018-11-10 15:00:00 +0900", "snippet": "개요2017년 후반부터 2018년 초까지 팀 내 서비스들을 마이크로서비스 아키텍쳐 형태로 개발하는 프로젝트를 진행하였다.사내에서 L7 Switch를 제공하고 있었지만, 서비스가 추가될 때마다 요청하기도 번거롭고 Software Level Gateway에서만 할 수 있는 작업들도 여럿 있었다.당시에 Gateway 후보로 Spring Cloud Zuul를 검토했었는데, 아래 이유들 때문에 도입하지 않았었다. Spring Cloud Zuul이 사용하는 Service Discovery는 Spring Cloud Eureka 혹은 Pr..." } ]
